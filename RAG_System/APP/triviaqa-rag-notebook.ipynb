{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"%%writefile requirements.txt\ndatasets\ntransformers\nsentence-transformers\nchromadb\nlangchain\nlangchain-community\nlangchain-chroma\nlangchain-huggingface\nfastapi\nuvicorn\npython-dotenv\nbitsandbytes\naccelerate\nnest-asyncio\ntorch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:13:54.303352Z","iopub.execute_input":"2025-12-27T00:13:54.303564Z","iopub.status.idle":"2025-12-27T00:13:54.312486Z","shell.execute_reply.started":"2025-12-27T00:13:54.303540Z","shell.execute_reply":"2025-12-27T00:13:54.311846Z"}},"outputs":[{"name":"stdout","text":"Writing requirements.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-12-27T00:13:54.313683Z","iopub.execute_input":"2025-12-27T00:13:54.313927Z","iopub.status.idle":"2025-12-27T00:14:21.668491Z","shell.execute_reply.started":"2025-12-27T00:13:54.313889Z","shell.execute_reply":"2025-12-27T00:14:21.667809Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (4.4.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.57.1)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (5.1.1)\nCollecting chromadb (from -r requirements.txt (line 4))\n  Downloading chromadb-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\nRequirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.3.27)\nCollecting langchain-community (from -r requirements.txt (line 6))\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting langchain-chroma (from -r requirements.txt (line 7))\n  Downloading langchain_chroma-1.1.0-py3-none-any.whl.metadata (1.9 kB)\nCollecting langchain-huggingface (from -r requirements.txt (line 8))\n  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.119.1)\nRequirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (0.38.0)\nRequirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (1.1.1)\nCollecting bitsandbytes (from -r requirements.txt (line 12))\n  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (1.11.0)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (1.6.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (2.8.0+cu126)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (3.20.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (2.0.2)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (0.28.1)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2025.10.0)\nRequirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 1)) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->-r requirements.txt (line 2)) (0.6.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.6.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.15.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 3)) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->-r requirements.txt (line 3)) (4.15.0)\nRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (1.3.0)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (2.12.5)\nCollecting pybase64>=1.4.1 (from chromadb->-r requirements.txt (line 4))\n  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\nCollecting posthog<6.0.0,>=2.4.0 (from chromadb->-r requirements.txt (line 4))\n  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb->-r requirements.txt (line 4))\n  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (1.37.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->-r requirements.txt (line 4))\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (1.37.0)\nCollecting pypika>=0.48.9 (from chromadb->-r requirements.txt (line 4))\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (7.7.0)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (6.5.2)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (1.75.1)\nCollecting bcrypt>=4.0.1 (from chromadb->-r requirements.txt (line 4))\n  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (0.20.0)\nRequirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (33.1.0)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (8.5.0)\nCollecting mmh3>=4.0.1 (from chromadb->-r requirements.txt (line 4))\n  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (3.11.3)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (14.2.0)\nRequirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb->-r requirements.txt (line 4)) (4.25.1)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain->-r requirements.txt (line 5)) (0.3.79)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain->-r requirements.txt (line 5)) (0.3.11)\nRequirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain->-r requirements.txt (line 5)) (0.4.37)\nCollecting SQLAlchemy<3,>=1.4 (from langchain->-r requirements.txt (line 5))\n  Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-community (from -r requirements.txt (line 6))\n  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r requirements.txt (line 6)) (3.13.2)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r requirements.txt (line 6)) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r requirements.txt (line 6)) (2.11.0)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community->-r requirements.txt (line 6)) (0.4.3)\nINFO: pip is looking at multiple versions of langchain-chroma to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-chroma (from -r requirements.txt (line 7))\n  Downloading langchain_chroma-1.0.0-py3-none-any.whl.metadata (1.9 kB)\n  Downloading langchain_chroma-0.2.6-py3-none-any.whl.metadata (1.1 kB)\nINFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-huggingface (from -r requirements.txt (line 8))\n  Downloading langchain_huggingface-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n  Downloading langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\nRequirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->-r requirements.txt (line 9)) (0.48.0)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn->-r requirements.txt (line 10)) (8.3.1)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->-r requirements.txt (line 10)) (0.16.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 13)) (5.9.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 15)) (3.4.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 6)) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 6)) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 6)) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 6)) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 6)) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 6)) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 6)) (1.22.0)\nRequirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 4)) (1.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r requirements.txt (line 6)) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r requirements.txt (line 6)) (0.9.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 1)) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 1)) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 1)) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 1)) (3.11)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 1)) (1.2.1rc0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 4)) (2025.9.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 4)) (0.37.0)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 4)) (0.27.1)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.38.0)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (1.9.0)\nRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.0.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (3.3.1)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.6.2)\nRequirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (0.10)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirements.txt (line 5)) (1.33)\nRequirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 5)) (1.0.0)\nRequirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 5)) (0.25.0)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4))\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (25.9.23)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (5.29.5)\nRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 4)) (8.7.0)\nRequirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4)) (1.71.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4))\n  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.39.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4))\n  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-sdk>=1.2.0 (from chromadb->-r requirements.txt (line 4))\n  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb->-r requirements.txt (line 4))\n  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 4))\n  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\nCollecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 4))\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 4)) (1.9.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 4)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 4)) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb->-r requirements.txt (line 4)) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 4)) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 4)) (2.19.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 5)) (3.2.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 15)) (1.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (1.5.4)\nCollecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\nCollecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (15.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 15)) (3.0.3)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (3.6.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (4.9.1)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 4)) (3.23.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain->-r requirements.txt (line 5)) (3.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 4)) (0.1.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community->-r requirements.txt (line 6)) (1.1.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4))\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (0.6.1)\nDownloading chromadb-1.4.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_chroma-0.2.6-py3-none-any.whl (12 kB)\nDownloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\nDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl (19 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=627fcf0b91fd7be669e8696c393fa1101eb2d236a6fb37066580ff77714f4e58\n  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\nSuccessfully built pypika\nInstalling collected packages: pypika, uvloop, SQLAlchemy, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, bitsandbytes, opentelemetry-exporter-otlp-proto-grpc, langchain-huggingface, chromadb, langchain-community, langchain-chroma\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 1.2.19\n    Uninstalling SQLAlchemy-1.2.19:\n      Successfully uninstalled SQLAlchemy-1.2.19\n  Attempting uninstall: opentelemetry-proto\n    Found existing installation: opentelemetry-proto 1.37.0\n    Uninstalling opentelemetry-proto-1.37.0:\n      Successfully uninstalled opentelemetry-proto-1.37.0\n  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.37.0\n    Uninstalling opentelemetry-api-1.37.0:\n      Successfully uninstalled opentelemetry-api-1.37.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.37.0\n    Uninstalling opentelemetry-sdk-1.37.0:\n      Successfully uninstalled opentelemetry-sdk-1.37.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nxmanager 0.7.1 requires sqlalchemy==1.2.19, but you have sqlalchemy 2.0.45 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed SQLAlchemy-2.0.45 backoff-2.2.1 bcrypt-5.0.0 bitsandbytes-0.49.0 chromadb-1.4.0 coloredlogs-15.0.1 httptools-0.7.1 humanfriendly-10.0 langchain-chroma-0.2.6 langchain-community-0.3.31 langchain-huggingface-0.3.1 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.39.1 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-exporter-otlp-proto-grpc-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 posthog-5.4.0 pybase64-1.4.3 pypika-0.48.9 uvloop-0.22.1 watchfiles-1.1.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Modules","metadata":{}},{"cell_type":"markdown","source":"## Embedding Model","metadata":{}},{"cell_type":"code","source":"%%writefile embedding_model.py\nimport os\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nhf_token = os.getenv(\"HF_TOKEN\")\n\nMODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembedder = HuggingFaceEmbeddings(\n    model_name=MODEL_NAME,\n    model_kwargs={'device': 'cuda'} \n)\n\nprint(f\"[INFO] Embedding model '{MODEL_NAME}' loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:14:37.593483Z","iopub.execute_input":"2025-12-27T00:14:37.594229Z","iopub.status.idle":"2025-12-27T00:14:37.599034Z","shell.execute_reply.started":"2025-12-27T00:14:37.594192Z","shell.execute_reply":"2025-12-27T00:14:37.598377Z"}},"outputs":[{"name":"stdout","text":"Writing embedding_model.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\n\nuser_secrets = UserSecretsClient()\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n\n!python embedding_model.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:14:40.636941Z","iopub.execute_input":"2025-12-27T00:14:40.637264Z","iopub.status.idle":"2025-12-27T00:15:17.717174Z","shell.execute_reply.started":"2025-12-27T00:14:40.637237Z","shell.execute_reply":"2025-12-27T00:15:17.716268Z"}},"outputs":[{"name":"stdout","text":"2025-12-27 00:14:55.765224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766794495.955741     135 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766794496.012874     135 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766794496.461165     135 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794496.461216     135 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794496.461220     135 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794496.461224     135 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nmodules.json: 100%|████████████████████████████| 349/349 [00:00<00:00, 2.95MB/s]\nconfig_sentence_transformers.json: 100%|███████| 116/116 [00:00<00:00, 1.13MB/s]\nREADME.md: 10.5kB [00:00, 4.54MB/s]\nsentence_bert_config.json: 100%|██████████████| 53.0/53.0 [00:00<00:00, 519kB/s]\nconfig.json: 100%|█████████████████████████████| 612/612 [00:00<00:00, 7.51MB/s]\nmodel.safetensors: 100%|████████████████████| 90.9M/90.9M [00:00<00:00, 118MB/s]\ntokenizer_config.json: 100%|███████████████████| 350/350 [00:00<00:00, 3.55MB/s]\nvocab.txt: 232kB [00:00, 13.7MB/s]\ntokenizer.json: 466kB [00:00, 38.2MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 112/112 [00:00<00:00, 1.52MB/s]\nconfig.json: 100%|█████████████████████████████| 190/190 [00:00<00:00, 1.62MB/s]\n[INFO] Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## LLM Model","metadata":{}},{"cell_type":"code","source":"%%writefile llm_model.py\nimport torch\nimport os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nhf_token = os.getenv(\"HF_TOKEN\")\n\nLLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(f\"[INFO] Loading tokenizer for Mistral...\")\ntokenizer = AutoTokenizer.from_pretrained(LLM_MODEL, token=hf_token)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"[INFO] Loading Mistral-7B-v0.2 in 4-bit...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    LLM_MODEL,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    token=hf_token\n)\n\nprint(f\"[INFO] LLM model loaded successfully on {model.device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:15:17.719052Z","iopub.execute_input":"2025-12-27T00:15:17.719340Z","iopub.status.idle":"2025-12-27T00:15:17.724965Z","shell.execute_reply.started":"2025-12-27T00:15:17.719309Z","shell.execute_reply":"2025-12-27T00:15:17.724275Z"}},"outputs":[{"name":"stdout","text":"Writing llm_model.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n!python llm_model.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:15:17.725877Z","iopub.execute_input":"2025-12-27T00:15:17.726167Z","iopub.status.idle":"2025-12-27T00:16:44.545611Z","shell.execute_reply.started":"2025-12-27T00:15:17.726111Z","shell.execute_reply":"2025-12-27T00:16:44.544651Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"[INFO] Loading tokenizer for Mistral...\ntokenizer_config.json: 2.10kB [00:00, 9.26MB/s]\ntokenizer.model: 100%|███████████████████████| 493k/493k [00:00<00:00, 1.72MB/s]\ntokenizer.json: 1.80MB [00:00, 40.3MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 3.85MB/s]\n[INFO] Loading Mistral-7B-v0.2 in 4-bit...\nconfig.json: 100%|█████████████████████████████| 596/596 [00:00<00:00, 5.23MB/s]\n2025-12-27 00:15:25.551240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766794525.572580     164 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766794525.579411     164 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766794525.596220     164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794525.596247     164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794525.596252     164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794525.596259     164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nmodel.safetensors.index.json: 25.1kB [00:00, 91.5MB/s]\nFetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\nmodel-00001-of-00003.safetensors:   0%|             | 0.00/4.94G [00:00<?, ?B/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|             | 0.00/4.54G [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 754k/5.00G [00:01<1:59:58, 694kB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|    | 816k/4.54G [00:01<1:52:25, 673kB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   0%|    | 9.56M/5.00G [00:01<09:21, 8.89MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   1%|    | 27.8M/5.00G [00:01<04:18, 19.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   2%|    | 84.7M/5.00G [00:02<02:00, 40.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   5%|▎     | 243M/5.00G [00:03<00:43, 110MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 67.1M/4.94G [00:04<05:12, 15.6MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:   8%|▌     | 419M/5.00G [00:04<00:30, 150MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  10%|▌     | 502M/5.00G [00:04<00:29, 151MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   1%|    | 67.9M/4.54G [00:05<05:07, 14.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  14%|▊     | 678M/5.00G [00:05<00:19, 218MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   3%|▏    | 135M/4.54G [00:05<02:15, 32.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  15%|▉     | 745M/5.00G [00:05<00:18, 228MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   6%|▎    | 269M/4.54G [00:05<00:53, 79.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  16%|▉     | 812M/5.00G [00:05<00:16, 259MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  18%|█     | 896M/5.00G [00:05<00:14, 289MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   3%|▏    | 135M/4.94G [00:06<03:17, 24.3MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  19%|█▏    | 963M/5.00G [00:06<00:18, 222MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|▎    | 269M/4.94G [00:06<01:24, 55.4MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|▎    | 301M/4.94G [00:07<01:25, 54.4MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  21%|█    | 1.04G/5.00G [00:07<00:27, 142MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   7%|▎    | 338M/4.54G [00:07<01:13, 56.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  22%|█    | 1.08G/5.00G [00:07<00:28, 137MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   9%|▍    | 405M/4.54G [00:07<00:57, 72.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   7%|▎    | 368M/4.94G [00:07<01:12, 63.1MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  10%|▌    | 472M/4.54G [00:08<00:51, 79.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  10%|▍    | 476M/4.94G [00:08<00:54, 81.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  12%|▋     | 611M/4.94G [00:09<00:36, 119MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  22%|▉   | 1.11G/5.00G [00:09<00:57, 68.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  24%|▉   | 1.18G/5.00G [00:09<00:48, 78.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  12%|▌    | 539M/4.54G [00:09<01:01, 64.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  25%|█▎   | 1.26G/5.00G [00:10<00:36, 104MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  13%|▋    | 606M/4.54G [00:10<00:51, 75.8MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  15%|▉     | 739M/4.94G [00:10<00:38, 111MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  26%|█▎   | 1.30G/5.00G [00:10<00:35, 103MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  15%|▋    | 673M/4.54G [00:10<00:40, 95.4MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  16%|▉     | 806M/4.94G [00:10<00:32, 127MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  16%|▉     | 740M/4.54G [00:11<00:32, 117MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  21%|█▏    | 941M/4.54G [00:11<00:16, 225MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  18%|█     | 873M/4.94G [00:11<00:32, 127MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  27%|█   | 1.33G/5.00G [00:11<00:50, 73.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  24%|█▏   | 1.08G/4.54G [00:11<00:13, 266MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  28%|█   | 1.39G/5.00G [00:11<00:36, 99.8MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  29%|█▍   | 1.46G/5.00G [00:12<00:30, 116MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  30%|█▍   | 1.49G/5.00G [00:12<00:30, 114MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  27%|█▎   | 1.21G/4.54G [00:12<00:15, 210MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  19%|▉    | 941M/4.94G [00:12<00:42, 94.1MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  20%|▊   | 1.01G/4.94G [00:13<00:46, 85.5MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  30%|█▍   | 1.34G/4.54G [00:13<00:19, 166MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  32%|█▌   | 1.61G/5.00G [00:13<00:32, 104MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  31%|█▌   | 1.41G/4.54G [00:13<00:17, 180MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  24%|█▏   | 1.21G/4.94G [00:14<00:24, 150MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  33%|█▋   | 1.67G/5.00G [00:14<00:28, 115MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  33%|█▋   | 1.48G/4.54G [00:14<00:16, 186MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  26%|█▎   | 1.28G/4.94G [00:14<00:25, 146MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  34%|█▋   | 1.54G/4.54G [00:14<00:17, 170MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  34%|█▎  | 1.70G/5.00G [00:14<00:35, 92.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  35%|█▊   | 1.61G/4.54G [00:14<00:15, 190MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  35%|█▍  | 1.74G/5.00G [00:15<00:32, 99.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  31%|█▌   | 1.54G/4.94G [00:15<00:15, 226MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  37%|█▊   | 1.68G/4.54G [00:15<00:15, 184MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  33%|█▋   | 1.61G/4.94G [00:15<00:16, 208MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  40%|█▉   | 1.81G/4.54G [00:15<00:10, 248MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  35%|█▍  | 1.76G/5.00G [00:15<00:41, 78.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  41%|██   | 1.88G/4.54G [00:15<00:09, 279MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  35%|█▍  | 1.77G/5.00G [00:16<00:50, 63.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  34%|█▋   | 1.68G/4.94G [00:16<00:18, 174MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  44%|██▏  | 2.01G/4.54G [00:16<00:08, 285MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  37%|█▍  | 1.84G/5.00G [00:16<00:33, 94.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  35%|█▊   | 1.75G/4.94G [00:16<00:18, 172MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  46%|██▎  | 2.08G/4.54G [00:16<00:11, 206MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  38%|█▌  | 1.91G/5.00G [00:17<00:34, 90.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  39%|█▌  | 1.97G/5.00G [00:17<00:30, 98.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  47%|██▎  | 2.15G/4.54G [00:18<00:19, 123MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  37%|█▍  | 1.81G/4.94G [00:18<00:38, 82.3MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  50%|██▌  | 2.28G/4.54G [00:19<00:17, 131MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  39%|█▉   | 1.95G/4.94G [00:19<00:27, 109MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  52%|██▌  | 2.35G/4.54G [00:19<00:17, 123MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  55%|██▋  | 2.49G/4.54G [00:20<00:16, 127MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  59%|██▉  | 2.69G/4.54G [00:21<00:11, 166MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  64%|███▏ | 2.89G/4.54G [00:22<00:07, 210MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  42%|█▋  | 2.12G/5.00G [00:22<00:59, 48.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  65%|███▎ | 2.95G/4.54G [00:22<00:08, 179MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  44%|█▊  | 2.21G/5.00G [00:22<00:46, 59.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  41%|█▋  | 2.01G/4.94G [00:23<00:56, 51.9MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  45%|█▊  | 2.25G/5.00G [00:23<00:45, 59.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  42%|█▋  | 2.08G/4.94G [00:23<00:47, 60.6MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  67%|███▎ | 3.02G/4.54G [00:24<00:13, 114MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  47%|█▉  | 2.36G/5.00G [00:24<00:34, 76.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  43%|█▋  | 2.15G/4.94G [00:24<00:42, 65.2MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  48%|█▉  | 2.38G/5.00G [00:24<00:36, 72.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  70%|███▍ | 3.16G/4.54G [00:25<00:11, 123MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  50%|██▌  | 2.52G/5.00G [00:25<00:22, 112MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  52%|██▌  | 2.60G/5.00G [00:26<00:21, 109MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  45%|█▊  | 2.22G/4.94G [00:26<00:49, 55.5MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  53%|██▋  | 2.67G/5.00G [00:26<00:17, 132MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  46%|█▊  | 2.28G/4.94G [00:26<00:37, 70.5MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  54%|██▋  | 2.70G/5.00G [00:26<00:18, 127MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  49%|██▍  | 2.42G/4.94G [00:26<00:22, 111MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  71%|██▊ | 3.22G/4.54G [00:26<00:15, 87.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  50%|██▌  | 2.48G/4.94G [00:27<00:20, 122MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  72%|██▉ | 3.29G/4.54G [00:27<00:14, 87.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  55%|██▏ | 2.76G/5.00G [00:27<00:24, 91.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  53%|██▋  | 2.62G/4.94G [00:28<00:17, 135MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  74%|██▉ | 3.35G/4.54G [00:28<00:12, 98.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  57%|██▊  | 2.83G/5.00G [00:28<00:18, 115MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  54%|██▋  | 2.68G/4.94G [00:28<00:14, 160MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  58%|██▉  | 2.90G/5.00G [00:28<00:15, 134MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  75%|███▊ | 3.42G/4.54G [00:28<00:10, 104MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  60%|██▉  | 2.98G/5.00G [00:28<00:11, 175MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  56%|██▊  | 2.75G/4.94G [00:28<00:13, 163MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  78%|███▉ | 3.56G/4.54G [00:29<00:06, 145MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  57%|██▊  | 2.82G/4.94G [00:29<00:15, 140MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  80%|███▉ | 3.62G/4.54G [00:29<00:05, 158MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  61%|███  | 3.06G/5.00G [00:30<00:19, 102MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  58%|██▎ | 2.89G/4.94G [00:30<00:21, 97.1MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  62%|██▍ | 3.10G/5.00G [00:30<00:21, 90.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  63%|██▌ | 3.13G/5.00G [00:31<00:23, 80.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  60%|██▍ | 2.95G/4.94G [00:31<00:23, 85.6MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  81%|███▏| 3.69G/4.54G [00:31<00:11, 73.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  61%|██▍ | 3.02G/4.94G [00:32<00:22, 84.7MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  83%|███▎| 3.76G/4.54G [00:32<00:10, 77.9MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  62%|██▍ | 3.09G/4.94G [00:32<00:18, 98.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  64%|██▌ | 3.15G/4.94G [00:34<00:24, 72.0MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  64%|██▌ | 3.19G/5.00G [00:34<00:44, 40.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  65%|██▌ | 3.22G/4.94G [00:35<00:22, 76.3MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  64%|██▌ | 3.22G/5.00G [00:35<00:46, 38.2MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  65%|██▌ | 3.23G/5.00G [00:35<00:45, 38.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  67%|██▋ | 3.29G/4.94G [00:35<00:18, 91.1MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  69%|███▍ | 3.43G/4.94G [00:35<00:11, 132MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  66%|██▌ | 3.28G/5.00G [00:36<00:35, 48.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  66%|██▋ | 3.29G/5.00G [00:36<00:38, 44.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  66%|██▋ | 3.30G/5.00G [00:37<00:48, 34.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  72%|███▌ | 3.56G/4.94G [00:37<00:11, 121MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  69%|███▍ | 3.43G/5.00G [00:37<00:15, 100MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  74%|███▋ | 3.64G/4.94G [00:37<00:09, 142MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  69%|███▍ | 3.46G/5.00G [00:37<00:13, 113MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  75%|███▊ | 3.71G/4.94G [00:37<00:08, 151MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  76%|███▊ | 3.78G/4.94G [00:38<00:06, 176MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  84%|███▎| 3.82G/4.54G [00:38<00:23, 30.8MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  79%|███▉ | 3.91G/4.94G [00:38<00:04, 247MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  80%|████ | 3.98G/4.94G [00:38<00:04, 219MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  71%|██▊ | 3.55G/5.00G [00:38<00:17, 83.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  82%|████ | 4.05G/4.94G [00:39<00:04, 217MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  83%|████▏| 4.11G/4.94G [00:39<00:03, 241MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  72%|███▌ | 3.61G/5.00G [00:39<00:13, 100MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  86%|████▎| 4.25G/4.94G [00:39<00:02, 316MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  73%|██▉ | 3.64G/5.00G [00:39<00:14, 95.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  87%|████▎| 4.31G/4.94G [00:39<00:02, 311MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  74%|███▋ | 3.71G/5.00G [00:39<00:10, 126MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  76%|███▊ | 3.80G/5.00G [00:40<00:07, 164MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  77%|███▊ | 3.84G/5.00G [00:40<00:07, 147MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  77%|███▊ | 3.86G/5.00G [00:40<00:07, 154MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  86%|███▍| 3.89G/4.54G [00:40<00:23, 28.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  79%|███▉ | 3.94G/5.00G [00:41<00:06, 155MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  89%|████▍| 4.38G/4.94G [00:41<00:05, 111MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  81%|████ | 4.03G/5.00G [00:41<00:05, 185MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  82%|████ | 4.10G/5.00G [00:41<00:04, 211MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  87%|███▍| 3.94G/4.54G [00:41<00:19, 31.0MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  88%|███▌| 4.00G/4.54G [00:42<00:12, 43.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  83%|████▏| 4.15G/5.00G [00:42<00:04, 177MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  85%|████▎| 4.26G/5.00G [00:42<00:03, 209MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  90%|███▌| 4.45G/4.94G [00:43<00:06, 77.2MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  90%|███▌| 4.07G/4.54G [00:43<00:09, 47.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  91%|███▋| 4.50G/4.94G [00:43<00:05, 88.8MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  86%|████▎| 4.29G/5.00G [00:43<00:06, 108MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  87%|████▎| 4.34G/5.00G [00:43<00:05, 126MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  91%|███▋| 4.14G/4.54G [00:44<00:07, 52.7MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  88%|████▍| 4.38G/5.00G [00:44<00:05, 108MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  94%|███▋| 4.63G/4.94G [00:44<00:03, 94.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  95%|████▋| 4.68G/4.94G [00:44<00:02, 105MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  94%|███▊| 4.27G/4.54G [00:45<00:03, 72.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  89%|███▌| 4.44G/5.00G [00:45<00:06, 89.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  90%|████▍| 4.49G/5.00G [00:45<00:04, 108MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  96%|███▊| 4.75G/4.94G [00:45<00:02, 98.7MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  96%|███▊| 4.34G/4.54G [00:46<00:02, 71.6MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  97%|███▉| 4.81G/4.94G [00:46<00:01, 98.4MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  97%|███▉| 4.41G/4.54G [00:46<00:01, 86.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  91%|███▋| 4.56G/5.00G [00:47<00:06, 69.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  92%|███▋| 4.62G/5.00G [00:47<00:03, 95.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors: 100%|████| 4.54G/4.54G [00:47<00:00, 95.5MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00002-of-00003.safetensors:  94%|████▋| 4.69G/5.00G [00:47<00:02, 119MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  95%|████▋| 4.73G/5.00G [00:47<00:02, 120MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  99%|███▉| 4.88G/4.94G [00:48<00:01, 67.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors: 100%|█████| 4.94G/4.94G [00:48<00:00, 102MB/s]\u001b[A\nFetching 3 files:  33%|█████████                  | 1/3 [00:48<01:37, 48.55s/it]\n\n\nmodel-00002-of-00003.safetensors:  96%|████▊| 4.80G/5.00G [00:48<00:01, 137MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors:  97%|████▊| 4.87G/5.00G [00:48<00:00, 179MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00002-of-00003.safetensors: 100%|█████| 5.00G/5.00G [00:48<00:00, 103MB/s]\u001b[A\u001b[A\u001b[A\nFetching 3 files: 100%|███████████████████████████| 3/3 [00:48<00:00, 16.31s/it]\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:16<00:00,  5.42s/it]\ngeneration_config.json: 100%|███████████████████| 111/111 [00:00<00:00, 865kB/s]\n[INFO] LLM model loaded successfully on cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"%%writefile preprocess.py\nimport os\nimport json\nfrom datasets import load_dataset\ntry:\n    from embedding_model import embedder\n    from llm_model import tokenizer\n    print(\"[SUCCESS] Imported embedder and tokenizer\")\nexcept ImportError:\n    print(\"[ERROR] Could not find embedding_model.py or llm_model.py. Make sure to run those cells first!\")\n\nprint(\"Tokenizer loaded successfully\")\n\n\ndef load_triviaqa_subset(n_samples=700):\n    print(f\"[load_triviaqa_subset] Loading {n_samples} samples from TriviaQA...\")\n    dataset = load_dataset(\"mandarjoshi/trivia_qa\", \"unfiltered\", split=f\"train[:{n_samples}]\")\n    docs = []\n\n    for item in dataset:\n        sr = item.get(\"search_results\", {})\n        contexts = sr.get(\"search_context\", [])\n\n        for ctx in contexts:\n            if ctx and ctx.strip():\n                docs.append({\n                    \"doc_id\": item[\"question_id\"],\n                    \"text\": ctx.strip()\n                })\n\n    print(f\"[load_triviaqa_subset] Docs extracted: {len(docs)}\")\n    # if docs:\n    #     print(\"[load_triviaqa_subset] Example doc:\", docs[0])\n    return docs\n\nimport re\n\ndef clean_text(text, min_length=15):\n    if not text:\n        return None\n    cleaned = text.strip()\n    cleaned = re.sub(r'\\s+', ' ', cleaned)\n    if len(cleaned) < min_length:\n        return None\n\n    return cleaned\n\ndef chunk_text(text, min_tokens=50, max_tokens=256, overlap=50): \n    token_ids = tokenizer(text, add_special_tokens=False)[\"input_ids\"]\n    if len(token_ids) < max_tokens:\n       return [tokenizer.decode(token_ids)]\n    \n    chunks = []\n    start = 0\n    while start < len(token_ids):\n        end = start + max_tokens\n        chunk_ids = token_ids[start:end]\n        chunks.append(tokenizer.decode(chunk_ids))\n        start += max_tokens - overlap \n    return chunks\n\ndef preprocess_triviaqa(n_samples=700, min_tokens_chunk=50, max_tokens_chunk=256, overlap_chunk=50):\n    print(f\"[preprocess_triviaqa] Starting preprocessing for {n_samples} samples...\")\n    docs = load_triviaqa_subset(n_samples)\n    final_chunks = []\n\n    for idx, d in enumerate(docs):\n        cleaned = clean_text(d[\"text\"])\n        if not cleaned:\n            print(f\"[clean_text] Doc {d['doc_id']} skipped (too short)\")\n            continue\n\n        chunks = chunk_text(cleaned, min_tokens=min_tokens_chunk, max_tokens=max_tokens_chunk, overlap=overlap_chunk)\n        if not chunks:\n            print(f\"[chunk_text] Doc {d['doc_id']} produced 0 chunks\")\n            continue\n\n        for i, ch in enumerate(chunks):\n            final_chunks.append({\n                \"doc_id\": d[\"doc_id\"],\n                \"chunk_id\": i,\n                \"text\": ch\n            })\n        if idx < 3: \n            print(f\"[preprocess_triviaqa] Doc {d['doc_id']} → {len(chunks)} chunks\")\n\n    print(f\"[preprocess_triviaqa] Total chunks generated: {len(final_chunks)}\")\n    if final_chunks:\n        print(\"[preprocess_triviaqa] Example chunk:\", final_chunks[0])\n    return final_chunks\n\ndef store_chunks_metadata(final_chunks, output_file=\"chunks_metadata.json\"):\n    metadata_list = []\n\n    for chunk in final_chunks:\n        text = chunk.get(\"text\", \"\")\n        meta = {\n            \"doc_id\": chunk.get(\"doc_id\"),\n            \"chunk_id\": chunk.get(\"chunk_id\"),\n            \"text\": text,  \n            \"text_length\": len(text),\n            \"num_tokens\": len(tokenizer(text, add_special_tokens=False)[\"input_ids\"])\n        }\n        metadata_list.append(meta)\n    \n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        json.dump(metadata_list, f, ensure_ascii=False, indent=2)\n    \n    print(f\"[store_chunks_metadata] Stored metadata for {len(metadata_list)} chunks in {output_file}\")\n    return metadata_list\n\n\n\n\n#TEST\nif __name__ == \"__main__\":\n    print(\"[main] Running full preprocessing test...\")\n    final_chunks = preprocess_triviaqa(n_samples=700, min_tokens_chunk=50, max_tokens_chunk=256, overlap_chunk=50)\n\n    print(f\"[main] Test complete. Total chunks generated: {len(final_chunks)}\")\n    if final_chunks:\n        print(\"[main] Example chunk:\", final_chunks[0])\n    chunks_metadata = store_chunks_metadata(final_chunks)\n    print(chunks_metadata[:2])  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:16:44.547636Z","iopub.execute_input":"2025-12-27T00:16:44.547942Z","iopub.status.idle":"2025-12-27T00:16:44.555201Z","shell.execute_reply.started":"2025-12-27T00:16:44.547913Z","shell.execute_reply":"2025-12-27T00:16:44.554502Z"}},"outputs":[{"name":"stdout","text":"Writing preprocess.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport sys\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\nsys.path.append('/kaggle/working')\n\n!python preprocess.py","metadata":{"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-12-27T00:16:44.555929Z","iopub.execute_input":"2025-12-27T00:16:44.556265Z","iopub.status.idle":"2025-12-27T00:24:34.939510Z","shell.execute_reply.started":"2025-12-27T00:16:44.556226Z","shell.execute_reply":"2025-12-27T00:24:34.938823Z"}},"outputs":[{"name":"stdout","text":"2025-12-27 00:16:54.560258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766794614.581264     255 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766794614.588053     255 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766794614.604829     255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794614.604858     255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794614.604865     255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766794614.604870     255 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n[INFO] Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded\n[INFO] Loading tokenizer for Mistral...\n[INFO] Loading Mistral-7B-v0.2 in 4-bit...\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:21<00:00,  7.09s/it]\n[INFO] LLM model loaded successfully on cuda:0\n[SUCCESS] Imported embedder and tokenizer\nTokenizer loaded successfully\n[main] Running full preprocessing test...\n[preprocess_triviaqa] Starting preprocessing for 700 samples...\n[load_triviaqa_subset] Loading 700 samples from TriviaQA...\nREADME.md: 26.7kB [00:00, 89.4MB/s]\nResolving data files: 100%|██████████████████| 26/26 [00:00<00:00, 27748.58it/s]\nResolving data files: 100%|██████████████████| 47/47 [00:00<00:00, 33634.58it/s]\nDownloading data:   0%|                               | 0/47 [00:00<?, ?files/s]\nunfiltered/train-00000-of-00047.parquet:   0%|       | 0.00/215M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00000-of-00047.parquet:   0%| | 277k/215M [00:00<10:58, 326kB/s\u001b[A\nunfiltered/train-00000-of-00047.parquet: 100%|█| 215M/215M [00:01<00:00, 156MB/s\u001b[A\nDownloading data:   2%|▍                      | 1/47 [00:01<01:19,  1.73s/files]\nunfiltered/train-00001-of-00047.parquet:   0%|       | 0.00/279M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00001-of-00047.parquet:   4%| | 11.1M/279M [00:00<00:16, 16.5MB\u001b[A\nunfiltered/train-00001-of-00047.parquet:  28%|▎| 78.2M/279M [00:01<00:02, 89.4MB\u001b[A\nunfiltered/train-00001-of-00047.parquet: 100%|█| 279M/279M [00:01<00:00, 242MB/s\u001b[A\nDownloading data:   4%|▉                      | 2/47 [00:03<01:07,  1.49s/files]\nunfiltered/train-00002-of-00047.parquet:   0%|       | 0.00/250M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00002-of-00047.parquet:  19%|▏| 48.4M/250M [00:01<00:04, 46.7MB\u001b[A\nunfiltered/train-00002-of-00047.parquet: 100%|█| 250M/250M [00:01<00:00, 203MB/s\u001b[A\nDownloading data:   6%|█▍                     | 3/47 [00:04<01:05,  1.50s/files]\nunfiltered/train-00003-of-00047.parquet:   0%|       | 0.00/243M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00003-of-00047.parquet:  17%|▏| 41.4M/243M [00:00<00:04, 42.0MB\u001b[A\nunfiltered/train-00003-of-00047.parquet: 100%|█| 243M/243M [00:01<00:00, 213MB/s\u001b[A\nDownloading data:   9%|█▉                     | 4/47 [00:05<01:00,  1.42s/files]\nunfiltered/train-00004-of-00047.parquet:   0%|       | 0.00/224M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00004-of-00047.parquet:  10%| | 22.8M/224M [00:01<00:08, 22.4MB\u001b[A\nunfiltered/train-00004-of-00047.parquet: 100%|█| 224M/224M [00:01<00:00, 170MB/s\u001b[A\nDownloading data:  11%|██▍                    | 5/47 [00:07<01:00,  1.44s/files]\nunfiltered/train-00005-of-00047.parquet:   0%|       | 0.00/231M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00005-of-00047.parquet:  13%|▏| 30.3M/231M [00:00<00:06, 33.1MB\u001b[A\nunfiltered/train-00005-of-00047.parquet: 100%|█| 231M/231M [00:01<00:00, 202MB/s\u001b[A\nDownloading data:  13%|██▉                    | 6/47 [00:08<00:57,  1.39s/files]\nunfiltered/train-00006-of-00047.parquet:   0%|       | 0.00/247M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00006-of-00047.parquet:  18%|▏| 45.6M/247M [00:00<00:04, 48.0MB\u001b[A\nunfiltered/train-00006-of-00047.parquet: 100%|█| 247M/247M [00:01<00:00, 201MB/s\u001b[A\nDownloading data:  15%|███▍                   | 7/47 [00:10<00:55,  1.39s/files]\nunfiltered/train-00007-of-00047.parquet:   0%|       | 0.00/245M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00007-of-00047.parquet:  18%|▏| 44.2M/245M [00:00<00:04, 45.6MB\u001b[A\nunfiltered/train-00007-of-00047.parquet: 100%|█| 245M/245M [00:01<00:00, 214MB/s\u001b[A\nDownloading data:  17%|███▉                   | 8/47 [00:11<00:53,  1.36s/files]\nunfiltered/train-00008-of-00047.parquet:   0%|       | 0.00/244M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00008-of-00047.parquet:  18%|▏| 43.0M/244M [00:00<00:04, 44.3MB\u001b[A\nunfiltered/train-00008-of-00047.parquet: 100%|█| 244M/244M [00:01<00:00, 206MB/s\u001b[A\nDownloading data:  19%|████▍                  | 9/47 [00:12<00:51,  1.35s/files]\nunfiltered/train-00009-of-00047.parquet:   0%|       | 0.00/244M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00009-of-00047.parquet:  27%|▎| 67.1M/244M [00:01<00:03, 57.2MB\u001b[A\nunfiltered/train-00009-of-00047.parquet: 100%|█| 244M/244M [00:01<00:00, 190MB/s\u001b[A\nDownloading data:  21%|████▋                 | 10/47 [00:14<00:51,  1.38s/files]\nunfiltered/train-00010-of-00047.parquet:   0%|       | 0.00/410M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00010-of-00047.parquet:   2%| | 7.88M/410M [00:01<00:53, 7.58MB\u001b[A\nunfiltered/train-00010-of-00047.parquet:  18%|▏| 75.0M/410M [00:01<00:05, 62.7MB\u001b[A\nunfiltered/train-00010-of-00047.parquet:  35%|▎| 142M/410M [00:01<00:02, 127MB/s\u001b[A\nunfiltered/train-00010-of-00047.parquet:  51%|▌| 209M/410M [00:01<00:01, 185MB/s\u001b[A\nunfiltered/train-00010-of-00047.parquet: 100%|█| 410M/410M [00:01<00:00, 218MB/s\u001b[A\nDownloading data:  23%|█████▏                | 11/47 [00:16<00:57,  1.61s/files]\nunfiltered/train-00011-of-00047.parquet:   0%|       | 0.00/386M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00011-of-00047.parquet:  17%|▏| 67.0M/386M [00:01<00:05, 54.3MB\u001b[A\nunfiltered/train-00011-of-00047.parquet:  48%|▍| 185M/386M [00:01<00:01, 167MB/s\u001b[A\nunfiltered/train-00011-of-00047.parquet: 100%|█| 386M/386M [00:01<00:00, 251MB/s\u001b[A\nDownloading data:  26%|█████▌                | 12/47 [00:17<00:57,  1.64s/files]\nunfiltered/train-00012-of-00047.parquet:   0%|       | 0.00/367M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00012-of-00047.parquet:   9%| | 31.9M/367M [00:00<00:09, 34.4MB\u001b[A\nunfiltered/train-00012-of-00047.parquet:  27%|▎| 98.9M/367M [00:01<00:02, 97.6MB\u001b[A\nunfiltered/train-00012-of-00047.parquet: 100%|█| 367M/367M [00:01<00:00, 268MB/s\u001b[A\nDownloading data:  28%|██████                | 13/47 [00:19<00:54,  1.60s/files]\nunfiltered/train-00013-of-00047.parquet:   0%|       | 0.00/350M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00013-of-00047.parquet:   4%| | 14.2M/350M [00:00<00:18, 18.1MB\u001b[A\nunfiltered/train-00013-of-00047.parquet:  23%|▏| 81.3M/350M [00:01<00:03, 83.3MB\u001b[A\nunfiltered/train-00013-of-00047.parquet:  62%|▌| 215M/350M [00:01<00:00, 248MB/s\u001b[A\nunfiltered/train-00013-of-00047.parquet: 100%|█| 350M/350M [00:01<00:00, 245MB/s\u001b[A\nDownloading data:  30%|██████▌               | 14/47 [00:21<00:52,  1.60s/files]\nunfiltered/train-00014-of-00047.parquet:   0%|       | 0.00/310M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00014-of-00047.parquet:  13%|▏| 41.5M/310M [00:01<00:07, 37.9MB\u001b[A\nunfiltered/train-00014-of-00047.parquet:  35%|▎| 108M/310M [00:01<00:01, 103MB/s\u001b[A\nunfiltered/train-00014-of-00047.parquet: 100%|█| 310M/310M [00:01<00:00, 225MB/s\u001b[A\nDownloading data:  32%|███████               | 15/47 [00:22<00:50,  1.58s/files]\nunfiltered/train-00015-of-00047.parquet:   0%|       | 0.00/336M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00015-of-00047.parquet:   0%| | 212k/336M [00:00<17:11, 325kB/s\u001b[A\nunfiltered/train-00015-of-00047.parquet:  20%|▏| 67.3M/336M [00:01<00:03, 71.5MB\u001b[A\nunfiltered/train-00015-of-00047.parquet: 100%|█| 336M/336M [00:01<00:00, 257MB/s\u001b[A\nDownloading data:  34%|███████▍              | 16/47 [00:24<00:48,  1.56s/files]\nunfiltered/train-00016-of-00047.parquet:   0%|       | 0.00/397M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00016-of-00047.parquet:  17%|▏| 67.1M/397M [00:01<00:06, 48.6MB\u001b[A\nunfiltered/train-00016-of-00047.parquet:  66%|▋| 263M/397M [00:01<00:00, 223MB/s\u001b[A\nunfiltered/train-00016-of-00047.parquet: 100%|█| 397M/397M [00:01<00:00, 245MB/s\u001b[A\nDownloading data:  36%|███████▉              | 17/47 [00:25<00:49,  1.64s/files]\nunfiltered/train-00017-of-00047.parquet:   0%|       | 0.00/372M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00017-of-00047.parquet:  10%| | 37.0M/372M [00:00<00:08, 38.7MB\u001b[A\nunfiltered/train-00017-of-00047.parquet:  28%|▎| 104M/372M [00:01<00:02, 104MB/s\u001b[A\nunfiltered/train-00017-of-00047.parquet: 100%|█| 372M/372M [00:01<00:00, 276MB/s\u001b[A\nDownloading data:  38%|████████▍             | 18/47 [00:27<00:46,  1.60s/files]\nunfiltered/train-00018-of-00047.parquet:   0%|       | 0.00/320M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00018-of-00047.parquet:  21%|▏| 67.0M/320M [00:01<00:04, 62.2MB\u001b[A\nunfiltered/train-00018-of-00047.parquet: 100%|█| 320M/320M [00:01<00:00, 257MB/s\u001b[A\nDownloading data:  40%|████████▉             | 19/47 [00:28<00:43,  1.55s/files]\nunfiltered/train-00019-of-00047.parquet:   0%|       | 0.00/351M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00019-of-00047.parquet:   4%| | 15.5M/351M [00:00<00:16, 19.9MB\u001b[A\nunfiltered/train-00019-of-00047.parquet:  24%|▏| 82.5M/351M [00:01<00:03, 81.5MB\u001b[A\nunfiltered/train-00019-of-00047.parquet: 100%|█| 351M/351M [00:01<00:00, 256MB/s\u001b[A\nDownloading data:  43%|█████████▎            | 20/47 [00:30<00:41,  1.54s/files]\nunfiltered/train-00020-of-00047.parquet:   0%|       | 0.00/314M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00020-of-00047.parquet:  14%|▏| 45.3M/314M [00:01<00:07, 34.6MB\u001b[A\nunfiltered/train-00020-of-00047.parquet:  36%|▎| 112M/314M [00:01<00:02, 92.5MB/\u001b[A\nunfiltered/train-00020-of-00047.parquet: 100%|█| 314M/314M [00:01<00:00, 194MB/s\u001b[A\nDownloading data:  45%|█████████▊            | 21/47 [00:32<00:41,  1.61s/files]\nunfiltered/train-00021-of-00047.parquet:   0%|       | 0.00/270M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00021-of-00047.parquet:   1%| | 1.79M/270M [00:00<01:47, 2.50MB\u001b[A\nunfiltered/train-00021-of-00047.parquet:  26%|▎| 68.9M/270M [00:01<00:03, 59.2MB\u001b[A\nunfiltered/train-00021-of-00047.parquet: 100%|█| 270M/270M [00:01<00:00, 169MB/s\u001b[A\nDownloading data:  47%|██████████▎           | 22/47 [00:33<00:41,  1.65s/files]\nunfiltered/train-00022-of-00047.parquet:   0%|       | 0.00/201M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00022-of-00047.parquet:  33%|▎| 67.1M/201M [00:01<00:02, 50.7MB\u001b[A\nunfiltered/train-00022-of-00047.parquet: 100%|█| 201M/201M [00:01<00:00, 135MB/s\u001b[A\nDownloading data:  49%|██████████▊           | 23/47 [00:35<00:39,  1.65s/files]\nunfiltered/train-00023-of-00047.parquet:   0%|       | 0.00/244M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00023-of-00047.parquet:  17%|▏| 42.6M/244M [00:01<00:04, 41.6MB\u001b[A\nunfiltered/train-00023-of-00047.parquet: 100%|█| 244M/244M [00:01<00:00, 198MB/s\u001b[A\nDownloading data:  51%|███████████▏          | 24/47 [00:37<00:36,  1.61s/files]\nunfiltered/train-00024-of-00047.parquet:   0%|       | 0.00/271M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00024-of-00047.parquet:   1%| | 2.76M/271M [00:00<01:09, 3.87MB\u001b[A\nunfiltered/train-00024-of-00047.parquet:  26%|▎| 69.8M/271M [00:01<00:02, 73.4MB\u001b[A\nunfiltered/train-00024-of-00047.parquet: 100%|█| 271M/271M [00:01<00:00, 207MB/s\u001b[A\nDownloading data:  53%|███████████▋          | 25/47 [00:38<00:34,  1.56s/files]\nunfiltered/train-00025-of-00047.parquet:   0%|       | 0.00/252M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00025-of-00047.parquet:  27%|▎| 67.0M/252M [00:01<00:03, 56.1MB\u001b[A\nunfiltered/train-00025-of-00047.parquet: 100%|█| 252M/252M [00:01<00:00, 193MB/s\u001b[A\nDownloading data:  55%|████████████▏         | 26/47 [00:39<00:32,  1.53s/files]\nunfiltered/train-00026-of-00047.parquet:   0%|       | 0.00/278M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00026-of-00047.parquet:   4%| | 9.82M/278M [00:00<00:18, 14.9MB\u001b[A\nunfiltered/train-00026-of-00047.parquet:  28%|▎| 76.9M/278M [00:01<00:02, 83.5MB\u001b[A\nunfiltered/train-00026-of-00047.parquet: 100%|█| 278M/278M [00:01<00:00, 220MB/s\u001b[A\nDownloading data:  57%|████████████▋         | 27/47 [00:41<00:29,  1.50s/files]\nunfiltered/train-00027-of-00047.parquet:   0%|       | 0.00/258M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00027-of-00047.parquet:  22%|▏| 56.8M/258M [00:01<00:04, 48.5MB\u001b[A\nunfiltered/train-00027-of-00047.parquet: 100%|█| 258M/258M [00:01<00:00, 200MB/s\u001b[A\nDownloading data:  60%|█████████████         | 28/47 [00:42<00:28,  1.48s/files]\nunfiltered/train-00028-of-00047.parquet:   0%|       | 0.00/252M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00028-of-00047.parquet:  20%|▏| 50.3M/252M [00:00<00:03, 50.5MB\u001b[A\nunfiltered/train-00028-of-00047.parquet: 100%|█| 252M/252M [00:01<00:00, 216MB/s\u001b[A\nDownloading data:  62%|█████████████▌        | 29/47 [00:44<00:25,  1.43s/files]\nunfiltered/train-00029-of-00047.parquet:   0%|       | 0.00/261M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00029-of-00047.parquet:  26%|▎| 67.0M/261M [00:01<00:03, 56.0MB\u001b[A\nunfiltered/train-00029-of-00047.parquet: 100%|█| 261M/261M [00:01<00:00, 193MB/s\u001b[A\nDownloading data:  64%|██████████████        | 30/47 [00:45<00:24,  1.46s/files]\nunfiltered/train-00030-of-00047.parquet:   0%|       | 0.00/273M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00030-of-00047.parquet:   2%| | 4.63M/273M [00:00<00:36, 7.28MB\u001b[A\nunfiltered/train-00030-of-00047.parquet:  26%|▎| 71.7M/273M [00:01<00:02, 78.0MB\u001b[A\nunfiltered/train-00030-of-00047.parquet: 100%|█| 273M/273M [00:01<00:00, 227MB/s\u001b[A\nDownloading data:  66%|██████████████▌       | 31/47 [00:47<00:22,  1.43s/files]\nunfiltered/train-00031-of-00047.parquet:   0%|       | 0.00/264M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00031-of-00047.parquet:  24%|▏| 62.5M/264M [00:01<00:03, 56.8MB\u001b[A\nunfiltered/train-00031-of-00047.parquet: 100%|█| 264M/264M [00:01<00:00, 216MB/s\u001b[A\nDownloading data:  68%|██████████████▉       | 32/47 [00:48<00:21,  1.41s/files]\nunfiltered/train-00032-of-00047.parquet:   0%|       | 0.00/268M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00032-of-00047.parquet:   0%| | 276k/268M [00:00<10:22, 431kB/s\u001b[A\nunfiltered/train-00032-of-00047.parquet:  25%|▎| 67.3M/268M [00:01<00:02, 76.4MB\u001b[A\nunfiltered/train-00032-of-00047.parquet: 100%|█| 268M/268M [00:01<00:00, 203MB/s\u001b[A\nDownloading data:  70%|███████████████▍      | 33/47 [00:49<00:20,  1.46s/files]\nunfiltered/train-00033-of-00047.parquet:   0%|       | 0.00/269M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00033-of-00047.parquet:   0%| | 1.20M/269M [00:00<01:59, 2.25MB\u001b[A\nunfiltered/train-00033-of-00047.parquet:  25%|▎| 68.3M/269M [00:00<00:02, 82.2MB\u001b[A\nunfiltered/train-00033-of-00047.parquet: 100%|█| 269M/269M [00:01<00:00, 244MB/s\u001b[A\nDownloading data:  72%|███████████████▉      | 34/47 [00:51<00:18,  1.40s/files]\nunfiltered/train-00034-of-00047.parquet:   0%|       | 0.00/338M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00034-of-00047.parquet:   1%| | 2.80M/338M [00:00<01:23, 4.03MB\u001b[A\nunfiltered/train-00034-of-00047.parquet:  21%|▏| 69.8M/338M [00:01<00:03, 73.2MB\u001b[A\nunfiltered/train-00034-of-00047.parquet: 100%|█| 338M/338M [00:01<00:00, 235MB/s\u001b[A\nDownloading data:  74%|████████████████▍     | 35/47 [00:52<00:17,  1.46s/files]\nunfiltered/train-00035-of-00047.parquet:   0%|       | 0.00/350M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00035-of-00047.parquet:   4%| | 14.1M/350M [00:01<00:25, 13.4MB\u001b[A\nunfiltered/train-00035-of-00047.parquet:  23%|▏| 81.2M/350M [00:01<00:04, 65.5MB\u001b[A\nunfiltered/train-00035-of-00047.parquet:  62%|▌| 215M/350M [00:01<00:00, 201MB/s\u001b[A\nunfiltered/train-00035-of-00047.parquet: 100%|█| 350M/350M [00:01<00:00, 205MB/s\u001b[A\nDownloading data:  77%|████████████████▊     | 36/47 [00:54<00:17,  1.58s/files]\nunfiltered/train-00036-of-00047.parquet:   0%|       | 0.00/284M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00036-of-00047.parquet:   6%| | 16.2M/284M [00:00<00:12, 21.2MB\u001b[A\nunfiltered/train-00036-of-00047.parquet:  29%|▎| 83.2M/284M [00:01<00:02, 88.5MB\u001b[A\nunfiltered/train-00036-of-00047.parquet: 100%|█| 284M/284M [00:01<00:00, 231MB/s\u001b[A\nDownloading data:  79%|█████████████████▎    | 37/47 [00:56<00:15,  1.56s/files]\nunfiltered/train-00037-of-00047.parquet:   0%|       | 0.00/268M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00037-of-00047.parquet:   0%| | 18.9k/268M [00:00<2:03:42, 36.1\u001b[A\nunfiltered/train-00037-of-00047.parquet:  25%|▎| 67.0M/268M [00:00<00:02, 81.3MB\u001b[A\nunfiltered/train-00037-of-00047.parquet: 100%|█| 268M/268M [00:01<00:00, 244MB/s\u001b[A\nDownloading data:  81%|█████████████████▊    | 38/47 [00:57<00:13,  1.47s/files]\nunfiltered/train-00038-of-00047.parquet:   0%|       | 0.00/271M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00038-of-00047.parquet:   1%| | 2.92M/271M [00:00<01:13, 3.63MB\u001b[A\nunfiltered/train-00038-of-00047.parquet:  26%|▎| 70.0M/271M [00:01<00:02, 68.2MB\u001b[A\nunfiltered/train-00038-of-00047.parquet:  51%|▌| 137M/271M [00:01<00:01, 133MB/s\u001b[A\nunfiltered/train-00038-of-00047.parquet: 100%|█| 271M/271M [00:01<00:00, 177MB/s\u001b[A\nDownloading data:  83%|██████████████████▎   | 39/47 [00:59<00:12,  1.54s/files]\nunfiltered/train-00039-of-00047.parquet:   0%|       | 0.00/257M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00039-of-00047.parquet:  26%|▎| 67.0M/257M [00:01<00:03, 52.4MB\u001b[A\nunfiltered/train-00039-of-00047.parquet: 100%|█| 257M/257M [00:01<00:00, 182MB/s\u001b[A\nDownloading data:  85%|██████████████████▋   | 40/47 [01:00<00:10,  1.55s/files]\nunfiltered/train-00040-of-00047.parquet:   0%|       | 0.00/284M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00040-of-00047.parquet:   5%| | 15.4M/284M [00:01<00:20, 13.3MB\u001b[A\nunfiltered/train-00040-of-00047.parquet:  29%|▎| 82.5M/284M [00:01<00:02, 84.4MB\u001b[A\nunfiltered/train-00040-of-00047.parquet: 100%|█| 284M/284M [00:01<00:00, 192MB/s\u001b[A\nDownloading data:  87%|███████████████████▏  | 41/47 [01:02<00:09,  1.57s/files]\nunfiltered/train-00041-of-00047.parquet:   0%|       | 0.00/251M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00041-of-00047.parquet:  20%|▏| 50.3M/251M [00:01<00:05, 39.5MB\u001b[A\nunfiltered/train-00041-of-00047.parquet:  73%|▋| 184M/251M [00:01<00:00, 167MB/s\u001b[A\nunfiltered/train-00041-of-00047.parquet: 100%|█| 251M/251M [00:01<00:00, 160MB/s\u001b[A\nDownloading data:  89%|███████████████████▋  | 42/47 [01:04<00:08,  1.63s/files]\nunfiltered/train-00042-of-00047.parquet:   0%|       | 0.00/196M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00042-of-00047.parquet: 100%|█| 196M/196M [00:01<00:00, 164MB/s\u001b[A\nDownloading data:  91%|████████████████████▏ | 43/47 [01:05<00:06,  1.54s/files]\nunfiltered/train-00043-of-00047.parquet:   0%|       | 0.00/198M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00043-of-00047.parquet: 100%|█| 198M/198M [00:01<00:00, 188MB/s\u001b[A\nDownloading data:  94%|████████████████████▌ | 44/47 [01:06<00:04,  1.45s/files]\nunfiltered/train-00044-of-00047.parquet:   0%|       | 0.00/198M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00044-of-00047.parquet: 100%|█| 198M/198M [00:01<00:00, 146MB/s\u001b[A\nDownloading data:  96%|█████████████████████ | 45/47 [01:08<00:02,  1.47s/files]\nunfiltered/train-00045-of-00047.parquet:   0%|       | 0.00/349M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00045-of-00047.parquet:   4%| | 13.3M/349M [00:00<00:19, 17.2MB\u001b[A\nunfiltered/train-00045-of-00047.parquet:  23%|▏| 80.4M/349M [00:01<00:03, 83.3MB\u001b[A\nunfiltered/train-00045-of-00047.parquet:  62%|▌| 215M/349M [00:01<00:00, 251MB/s\u001b[A\nunfiltered/train-00045-of-00047.parquet: 100%|█| 349M/349M [00:01<00:00, 246MB/s\u001b[A\nDownloading data:  98%|█████████████████████▌| 46/47 [01:09<00:01,  1.50s/files]\nunfiltered/train-00046-of-00047.parquet:   0%|       | 0.00/361M [00:00<?, ?B/s]\u001b[A\nunfiltered/train-00046-of-00047.parquet:   7%| | 25.3M/361M [00:02<00:27, 12.0MB\u001b[A\nunfiltered/train-00046-of-00047.parquet:  26%|▎| 92.4M/361M [00:02<00:05, 50.9MB\u001b[A\nunfiltered/train-00046-of-00047.parquet: 100%|█| 361M/361M [00:02<00:00, 144MB/s\u001b[A\nDownloading data: 100%|██████████████████████| 47/47 [01:12<00:00,  1.54s/files]\nunfiltered/validation-00000-of-00007.par(…): 100%|█| 212M/212M [00:01<00:00, 151\nunfiltered/validation-00001-of-00007.par(…): 100%|█| 265M/265M [00:01<00:00, 202\nunfiltered/validation-00002-of-00007.par(…): 100%|█| 308M/308M [00:01<00:00, 240\nunfiltered/validation-00003-of-00007.par(…): 100%|█| 226M/226M [00:01<00:00, 163\nunfiltered/validation-00004-of-00007.par(…): 100%|█| 234M/234M [00:01<00:00, 154\nunfiltered/validation-00005-of-00007.par(…): 100%|█| 259M/259M [00:01<00:00, 224\nunfiltered/validation-00006-of-00007.par(…): 100%|█| 229M/229M [00:01<00:00, 190\nunfiltered/test-00000-of-00006.parquet: 100%|█| 235M/235M [00:01<00:00, 164MB/s]\nunfiltered/test-00001-of-00006.parquet: 100%|█| 316M/316M [00:01<00:00, 220MB/s]\nunfiltered/test-00002-of-00006.parquet: 100%|█| 300M/300M [00:01<00:00, 180MB/s]\nunfiltered/test-00003-of-00006.parquet: 100%|█| 266M/266M [00:01<00:00, 166MB/s]\nunfiltered/test-00004-of-00006.parquet: 100%|█| 295M/295M [00:01<00:00, 199MB/s]\nunfiltered/test-00005-of-00006.parquet: 100%|█| 251M/251M [00:01<00:00, 199MB/s]\nGenerating train split: 100%|█████| 87622/87622 [02:25<00:00, 602.21 examples/s]\nGenerating validation split: 100%|█| 11313/11313 [00:19<00:00, 591.82 examples/s\nGenerating test split: 100%|██████| 10832/10832 [00:19<00:00, 562.29 examples/s]\n[load_triviaqa_subset] Docs extracted: 5541\n[preprocess_triviaqa] Doc tc_0 → 41 chunks\n[preprocess_triviaqa] Doc tc_0 → 9 chunks\n[preprocess_triviaqa] Doc tc_0 → 20 chunks\n[preprocess_triviaqa] Total chunks generated: 128151\n[preprocess_triviaqa] Example chunk: {'doc_id': 'tc_0', 'chunk_id': 0, 'text': 'Peanuts | Peanuts Wiki | Fandom powered by Wikia Charles M. Schulz drawing Snoopy . Peanuts is a syndicated daily and Sunday comic strip written and illustrated by Charles M. Schulz , which ran from October 2, 1950, to February 13, 2000 (the day after Schulz\\'s death). In total 17,897 different Peanuts strips were published. The strip was one of the most popular and influential in the history of the medium, and considered the most beloved comic strips of all time. It was \"arguably the longest story ever told by one human being,\\'\"\\' according to Professor Robert Thompson of Syracuse University. At its peak, Peanuts ran in over 2,600 newspapers, with a readership of 355 million in 75 countries, and was translated into 21 languages. It helped to cement the four-panel gag strip as the standard in the United States. Reprints of the strip are still syndicated and run in many newspapers. In addition, Peanuts achieved considerable success for its television specials, several of which'}\n[main] Test complete. Total chunks generated: 128151\n[main] Example chunk: {'doc_id': 'tc_0', 'chunk_id': 0, 'text': 'Peanuts | Peanuts Wiki | Fandom powered by Wikia Charles M. Schulz drawing Snoopy . Peanuts is a syndicated daily and Sunday comic strip written and illustrated by Charles M. Schulz , which ran from October 2, 1950, to February 13, 2000 (the day after Schulz\\'s death). In total 17,897 different Peanuts strips were published. The strip was one of the most popular and influential in the history of the medium, and considered the most beloved comic strips of all time. It was \"arguably the longest story ever told by one human being,\\'\"\\' according to Professor Robert Thompson of Syracuse University. At its peak, Peanuts ran in over 2,600 newspapers, with a readership of 355 million in 75 countries, and was translated into 21 languages. It helped to cement the four-panel gag strip as the standard in the United States. Reprints of the strip are still syndicated and run in many newspapers. In addition, Peanuts achieved considerable success for its television specials, several of which'}\n[store_chunks_metadata] Stored metadata for 128151 chunks in chunks_metadata.json\n[{'doc_id': 'tc_0', 'chunk_id': 0, 'text': 'Peanuts | Peanuts Wiki | Fandom powered by Wikia Charles M. Schulz drawing Snoopy . Peanuts is a syndicated daily and Sunday comic strip written and illustrated by Charles M. Schulz , which ran from October 2, 1950, to February 13, 2000 (the day after Schulz\\'s death). In total 17,897 different Peanuts strips were published. The strip was one of the most popular and influential in the history of the medium, and considered the most beloved comic strips of all time. It was \"arguably the longest story ever told by one human being,\\'\"\\' according to Professor Robert Thompson of Syracuse University. At its peak, Peanuts ran in over 2,600 newspapers, with a readership of 355 million in 75 countries, and was translated into 21 languages. It helped to cement the four-panel gag strip as the standard in the United States. Reprints of the strip are still syndicated and run in many newspapers. In addition, Peanuts achieved considerable success for its television specials, several of which', 'text_length': 988, 'num_tokens': 256}, {'doc_id': 'tc_0', 'chunk_id': 1, 'text': \"the four-panel gag strip as the standard in the United States. Reprints of the strip are still syndicated and run in many newspapers. In addition, Peanuts achieved considerable success for its television specials, several of which, including A Charlie Brown Christmas and It's the Great Pumpkin, Charlie Brown won or were nominated for Emmys. The holiday specials remain quite popular to this day, and are currently broadcast on ABC in the United States during the appropriate season. Contents Peanuts 1940s Front cover of a collection of Li'l Folks comic strips. Peanuts had its origin in Li'l Folks , a weekly panel comic that appeared in Schulz's hometown paper, the St. Paul Pioneer Press, from 1947 to 1950. He first used the name Charlie Brown for a character there, although he used the name on four different occasions in Li'l Folks for different boys. The series also featured a dog that looked a lot like Snoopy . In 1948, Schulz sold a cartoon to the Saturday Evening Post; seventeen single-panel cartoons by Schulz would\", 'text_length': 1032, 'num_tokens': 256}]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Analyzes document length before and after chunking","metadata":{}},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom preprocess import load_triviaqa_subset\n\n\n# Load documents\nraw_docs = load_triviaqa_subset(n_samples=700)\n\noriginal_lengths = [\n    len(doc[\"text\"].split())\n    for doc in raw_docs\n]\norig_p50 = np.percentile(original_lengths, 50)\norig_p90 = np.percentile(original_lengths, 90)\norig_max = np.max(original_lengths)\n\nprint(\"\\n---Original Paragraph Analysis (Before Chunking) ---\")\nprint(f\"Total Paragraphs: {len(original_lengths)}\")\nprint(f\"Median Length (P50): {orig_p50:.2f} words\")\nprint(f\"90th Percentile (P90): {orig_p90:.2f} words\")\nprint(f\"Max Paragraph Length: {orig_max} words\")\n\n\n# Load CHUNKED documents\nwith open(\"chunks_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n    chunks = json.load(f)\n\nchunk_word_counts = [\n    len(chunk[\"text\"].split())\n    for chunk in chunks\n]\n\nchunk_avg = np.mean(chunk_word_counts)\nchunk_p90 = np.percentile(chunk_word_counts, 90)\nchunk_max = np.max(chunk_word_counts)\n\nprint(\"\\n--- Chunk Statistics (After Chunking) ---\")\nprint(f\"Total Chunks: {len(chunks)}\")\nprint(f\"Average Words per Chunk: {chunk_avg:.2f}\")\nprint(f\"90th Percentile (P90): {chunk_p90:.2f} words\")\nprint(f\"Max Words in a Chunk: {chunk_max} words\")\n\n\n# Comparison Summary\nprint(\"\\n--- Chunking Effect Summary ---\")\nprint(f\"Original P90 → {orig_p90:.2f} words\")\nprint(f\"Chunked  P90 → {chunk_p90:.2f} words\")\nprint(f\"Reduction Factor ≈ {(orig_p90 / chunk_p90):.2f}x\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:36:05.960656Z","iopub.execute_input":"2025-12-27T00:36:05.961049Z","iopub.status.idle":"2025-12-27T00:36:56.180692Z","shell.execute_reply.started":"2025-12-27T00:36:05.961006Z","shell.execute_reply":"2025-12-27T00:36:56.179964Z"}},"outputs":[{"name":"stderr","text":"2025-12-27 00:36:17.150329: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766795777.176099      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766795777.183487      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766795777.212030      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766795777.212051      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766795777.212054      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766795777.212056      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"[INFO] Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded\n[INFO] Loading tokenizer for Mistral...\n[INFO] Loading Mistral-7B-v0.2 in 4-bit...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ada460f81b498e8ba2e223bb99a579"}},"metadata":{}},{"name":"stdout","text":"[INFO] LLM model loaded successfully on cuda:0\n[SUCCESS] Imported embedder and tokenizer\nTokenizer loaded successfully\n[load_triviaqa_subset] Loading 700 samples from TriviaQA...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef88501794c43f9a42c86ccbdf7841c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe7485b30a8415fb88b737b96801198"}},"metadata":{}},{"name":"stdout","text":"[load_triviaqa_subset] Docs extracted: 5541\n\n---Original Paragraph Analysis (Before Chunking) ---\nTotal Paragraphs: 5541\nMedian Length (P50): 1038.00 words\n90th Percentile (P90): 6098.00 words\nMax Paragraph Length: 89615 words\n\n--- Chunk Statistics (After Chunking) ---\nTotal Chunks: 128151\nAverage Words per Chunk: 158.20\n90th Percentile (P90): 193.00 words\nMax Words in a Chunk: 234 words\n\n--- Chunking Effect Summary ---\nOriginal P90 → 6098.00 words\nChunked  P90 → 193.00 words\nReduction Factor ≈ 31.60x\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Vector DB","metadata":{}},{"cell_type":"code","source":"# #Delete chroma_db\n# import shutil\n# import os\n# db_path = \"/kaggle/working/chroma_db\"\n\n# if os.path.exists(db_path):\n#     shutil.rmtree(db_path)\n#     print(f\"Deleted old database at {db_path}\")\n# else:\n#     print(\"No old database found, starting fresh!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T21:08:06.333152Z","iopub.execute_input":"2025-12-26T21:08:06.334426Z","iopub.status.idle":"2025-12-26T21:08:06.338334Z","shell.execute_reply.started":"2025-12-26T21:08:06.334393Z","shell.execute_reply":"2025-12-26T21:08:06.337643Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"%%writefile vector_store.py\nimport json\nimport uuid\nimport os\nimport shutil\nfrom langchain_community.vectorstores import Chroma\nfrom embedding_model import embedder \n\npersist_dir = \"/kaggle/working/chroma_db\"\nif os.path.exists(persist_dir):\n    shutil.rmtree(persist_dir)\n    print(f\"[INFO] Deleted old database at {persist_dir}\")\n\nif not os.path.exists(\"chunks_metadata.json\"):\n    print(\"[ERROR] chunks_metadata.json not found! Please run preprocess.py first.\")\nelse:\n    with open(\"chunks_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n        chunks = json.load(f)\n    print(f\"[INFO] Loaded {len(chunks)} chunks from JSON\")\n\n    vectorstore = Chroma(\n        persist_directory=persist_dir,\n        embedding_function=embedder\n    )\n\n    texts = [chunk[\"text\"] for chunk in chunks]\n    ids = [f'{chunk[\"doc_id\"]}_{chunk[\"chunk_id\"]}_{uuid.uuid4().hex}' for chunk in chunks]\n\n    print(f\"[INFO] Generating embeddings for {len(texts)} texts using GPU...\")\n    all_embeddings = embedder.embed_documents(texts)\n\n    batch_size = 2000\n    for start in range(0, len(texts), batch_size):\n        end = start + batch_size\n        batch_ids = ids[start:end]\n        batch_texts = texts[start:end]\n        batch_embeddings = all_embeddings[start:end]\n\n        vectorstore._collection.add(\n            ids=batch_ids,\n            documents=batch_texts,\n            embeddings=batch_embeddings\n        )\n        print(f\"[INFO] Stored batch {start}–{min(end, len(texts))} items\")\n\n    vectorstore.persist()\n    print(f\"[INFO] Stored total {len(texts)} embeddings in ChromaDB at '{persist_dir}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:36:56.182223Z","iopub.execute_input":"2025-12-27T00:36:56.182930Z","iopub.status.idle":"2025-12-27T00:36:56.189097Z","shell.execute_reply.started":"2025-12-27T00:36:56.182901Z","shell.execute_reply":"2025-12-27T00:36:56.188424Z"}},"outputs":[{"name":"stdout","text":"Writing vector_store.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n\n!python vector_store.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:36:56.190157Z","iopub.execute_input":"2025-12-27T00:36:56.190557Z","iopub.status.idle":"2025-12-27T00:45:46.124613Z","shell.execute_reply.started":"2025-12-27T00:36:56.190514Z","shell.execute_reply":"2025-12-27T00:45:46.123849Z"}},"outputs":[{"name":"stdout","text":"2025-12-27 00:37:02.695568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766795822.717804     471 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766795822.724588     471 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766795822.744476     471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766795822.744509     471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766795822.744514     471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766795822.744519     471 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n[INFO] Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded\n[INFO] Loaded 128151 chunks from JSON\n/kaggle/working/vector_store.py:20: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n  vectorstore = Chroma(\n[INFO] Generating embeddings for 128151 texts using GPU...\n[INFO] Stored batch 0–2000 items\n[INFO] Stored batch 2000–4000 items\n[INFO] Stored batch 4000–6000 items\n[INFO] Stored batch 6000–8000 items\n[INFO] Stored batch 8000–10000 items\n[INFO] Stored batch 10000–12000 items\n[INFO] Stored batch 12000–14000 items\n[INFO] Stored batch 14000–16000 items\n[INFO] Stored batch 16000–18000 items\n[INFO] Stored batch 18000–20000 items\n[INFO] Stored batch 20000–22000 items\n[INFO] Stored batch 22000–24000 items\n[INFO] Stored batch 24000–26000 items\n[INFO] Stored batch 26000–28000 items\n[INFO] Stored batch 28000–30000 items\n[INFO] Stored batch 30000–32000 items\n[INFO] Stored batch 32000–34000 items\n[INFO] Stored batch 34000–36000 items\n[INFO] Stored batch 36000–38000 items\n[INFO] Stored batch 38000–40000 items\n[INFO] Stored batch 40000–42000 items\n[INFO] Stored batch 42000–44000 items\n[INFO] Stored batch 44000–46000 items\n[INFO] Stored batch 46000–48000 items\n[INFO] Stored batch 48000–50000 items\n[INFO] Stored batch 50000–52000 items\n[INFO] Stored batch 52000–54000 items\n[INFO] Stored batch 54000–56000 items\n[INFO] Stored batch 56000–58000 items\n[INFO] Stored batch 58000–60000 items\n[INFO] Stored batch 60000–62000 items\n[INFO] Stored batch 62000–64000 items\n[INFO] Stored batch 64000–66000 items\n[INFO] Stored batch 66000–68000 items\n[INFO] Stored batch 68000–70000 items\n[INFO] Stored batch 70000–72000 items\n[INFO] Stored batch 72000–74000 items\n[INFO] Stored batch 74000–76000 items\n[INFO] Stored batch 76000–78000 items\n[INFO] Stored batch 78000–80000 items\n[INFO] Stored batch 80000–82000 items\n[INFO] Stored batch 82000–84000 items\n[INFO] Stored batch 84000–86000 items\n[INFO] Stored batch 86000–88000 items\n[INFO] Stored batch 88000–90000 items\n[INFO] Stored batch 90000–92000 items\n[INFO] Stored batch 92000–94000 items\n[INFO] Stored batch 94000–96000 items\n[INFO] Stored batch 96000–98000 items\n[INFO] Stored batch 98000–100000 items\n[INFO] Stored batch 100000–102000 items\n[INFO] Stored batch 102000–104000 items\n[INFO] Stored batch 104000–106000 items\n[INFO] Stored batch 106000–108000 items\n[INFO] Stored batch 108000–110000 items\n[INFO] Stored batch 110000–112000 items\n[INFO] Stored batch 112000–114000 items\n[INFO] Stored batch 114000–116000 items\n[INFO] Stored batch 116000–118000 items\n[INFO] Stored batch 118000–120000 items\n[INFO] Stored batch 120000–122000 items\n[INFO] Stored batch 122000–124000 items\n[INFO] Stored batch 124000–126000 items\n[INFO] Stored batch 126000–128000 items\n[INFO] Stored batch 128000–128151 items\n/kaggle/working/vector_store.py:45: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n  vectorstore.persist()\n[INFO] Stored total 128151 embeddings in ChromaDB at '/kaggle/working/chroma_db'\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Test chroma_db Retrival","metadata":{}},{"cell_type":"code","source":"from langchain_community.vectorstores import Chroma\nfrom embedding_model import embedder\nimport os\n\npersist_dir = \"/kaggle/working/chroma_db\"\n\nif os.path.exists(persist_dir):\n    db = Chroma(persist_directory=persist_dir, embedding_function=embedder)\n    \n    query = \"Who wrote The Railway Children?\"\n    results = db.similarity_search(query, k=5) \n\n    print(f\"🔍 Searching for: '{query}'\\n\" + \"=\"*50)\n    for i, res in enumerate(results):\n        print(f\"📄 Result {i+1}:\")\n        print(f\"{res.page_content[:400]}...\")\n        print(\"-\" * 30)\nelse:\n    print(\"❌ Database not found! Please run 'python vector_store.py' first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T01:07:35.165931Z","iopub.execute_input":"2025-12-27T01:07:35.166606Z","iopub.status.idle":"2025-12-27T01:07:35.190349Z","shell.execute_reply.started":"2025-12-27T01:07:35.166570Z","shell.execute_reply":"2025-12-27T01:07:35.189722Z"}},"outputs":[{"name":"stdout","text":"🔍 Searching for: 'Who wrote The Railway Children?'\n==================================================\n📄 Result 1:\nconstructed film by John Schlesinger which explores the relationship between three people and the break-up of two love affairs. Peter Finch plays a homosexual doctor in his 40s and Glenda Jackson an employment counsellor in her 30s. Both are in love with Murray Head's boyish sculptor; he divides his attentions between both of them without showing a preference. Great performances all round. 66. The...\n------------------------------\n📄 Result 2:\nOne of six controversial, stylish Mitford sisters, she spied on her siblings for MI5 because of their Nazi sympathies. Iris Murdoch Novelist & philosopher, 1919-1999 Particularly admired in the Sixties and Seventies, the philosopher and novelist is considered one of the greatest post-war writers. She won the Booker Prize in 1978 for The Sea, with several of her other works adapted for the screen. ...\n------------------------------\n📄 Result 3:\nlo-American popular culture. Whether Lord Peter Wimsey, or Agatha Christie's Hercule Poirot and Miss Marple, to such American television detectives as Jessica Fletcher in Murder, She Wrote, the Holmes pattern set by Doyle at the turn of the century remain a dominant formula for audiences worldwide. Wednesday, January 12, 2000 John Bull and Uncle Sam The Railway Children (EMI,1970) Director: Lionel...\n------------------------------\n📄 Result 4:\non the New York subway. Technology Airplanes: October 10 — The Wright brothers experience their first crash. They rebuild the glider, and once again test it like a kite. Education Children’s Books: The best-known American children''s fantasy is undoubtedly \"The Wizard of Oz\" by L. Frank Baum (1856-1919). Education Crayola Crayons: The Company begins producing slate pencils for schools. Education P...\n------------------------------\n📄 Result 5:\nshare alike in the jubilation:— “For the spring-time has come, the May is here, On hill and in vale all is full of delight. How sweet is the spring-time, how lovely and bright,— Its kingdom is over us all.” The Erl King’s Daughter. “The Erl King’s Daughter” was written in 1852. Its story differs from that told in Goethe’s famous [148] poem, and set to music equally famous by Schubert in his famili...\n------------------------------\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Pipeline","metadata":{}},{"cell_type":"code","source":"%%writefile pipeline.py\nimport time\nimport re\nfrom transformers import pipeline\nfrom llm_model import model, tokenizer\nfrom embedding_model import embedder\nfrom langchain.chains import RetrievalQA\nfrom langchain_huggingface import HuggingFacePipeline\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.prompts import PromptTemplate\n\n\n# LLM pipeline \nraw_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=64,         \n    temperature=0.0,            \n    do_sample=False,\n    return_full_text=False\n)\n\nllm_pipeline = HuggingFacePipeline(pipeline=raw_pipeline)\n\n\n#vector DB\npersist_dir = \"/kaggle/working/chroma_db\"\nvector_db = Chroma(\n    persist_directory=persist_dir,\n    embedding_function=embedder\n)\n\nretriever = vector_db.as_retriever(search_kwargs={\"k\":5})\n\n\n# prompt \nprompt_template = \"\"\"<s>[INST]\nYou are a STRICT answer-only bot.\n\nRules:\n- Answer ONLY using the context\n- Answer must be very short (1–3 words)\n- If the answer is not explicitly present, reply EXACTLY with:\nNot found in context\n- No explanations\n- No full sentences\n\nContext:\n{context}\n\nQuestion:\n{question}\n[/INST]\nAnswer:\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template\n)\n\n\n# QA CHAIN  \nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm_pipeline,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    chain_type_kwargs={\"prompt\": prompt},\n)\n\n\n#  RAG Function\ndef run_rag(question: str):\n    start_time = time.time()\n    if question:\n        question = question.strip()  \n        question = re.sub(r'\\s+', ' ', question)  \n\n    # Handle empty or None queries \n    if not question or not question.strip():\n        latency_ms = int((time.time() - start_time) * 1000)\n        print(f\"[WARNING] Empty or None query received. Latency: {latency_ms}ms\")\n        return {\n            \"question\": question,\n            \"answer\": \"Empty query received\",\n            \"retrieved_context\": \"\",\n            \"latency_ms\": latency_ms\n        }\n\n    try:\n        # Retrieve context\n        docs = retriever.invoke(question)\n        context_string = \"\\n\".join(doc.page_content.strip() for doc in docs)\n\n        # Generate answer\n        response = qa_chain.invoke({\"query\": question})\n        raw_answer = response[\"result\"].strip()\n        answer = raw_answer.split(\"\\n\")[0].strip()\n\n        # Clean answer \n        if (\n            not answer\n            or len(answer.split()) > 3\n            or \"not found\" in answer.lower()\n            or \"context\" in answer.lower()\n            or \"does not\" in answer.lower()\n        ):\n            final_answer = \"Not found in context\"\n        else:\n            final_answer = answer.rstrip(\".\")\n\n        latency_ms = int((time.time() - start_time) * 1000)\n        return {\n            \"question\": question,\n            \"answer\": final_answer,\n            \"retrieved_context\": context_string,\n            \"latency_ms\": latency_ms\n        }\n\n    except Exception as e:\n        latency_ms = int((time.time() - start_time) * 1000)\n        print(f\"[ERROR] Exception during processing: {str(e)}. Latency: {latency_ms}ms\")\n        return {\n            \"question\": question,\n            \"answer\": \"Not found in context\",\n            \"retrieved_context\": \"\",\n            \"latency_ms\": latency_ms\n        }\n\n\n# TEST \nif __name__ == \"__main__\":\n    query = \"Miami Beach in Florida borders which ocean?\"\n    result = run_rag(query)\n    \n    print(\"--- RAG Result ---\")\n    print(f\"Question: {result['question']}\")\n    print(f\"Answer: {result['answer']}\")\n    print(f\"Latency: {result['latency_ms']} ms\")\n    print(\"\\n--- Context Found (Snippets) ---\")\n    print(result[\"retrieved_context\"][:500])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:45:48.445677Z","iopub.execute_input":"2025-12-27T00:45:48.446024Z","iopub.status.idle":"2025-12-27T00:45:48.452504Z","shell.execute_reply.started":"2025-12-27T00:45:48.445982Z","shell.execute_reply":"2025-12-27T00:45:48.451798Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"HF_TOKEN\")\n\n!python pipeline.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:45:48.453357Z","iopub.execute_input":"2025-12-27T00:45:48.453587Z","iopub.status.idle":"2025-12-27T00:46:35.483758Z","shell.execute_reply.started":"2025-12-27T00:45:48.453563Z","shell.execute_reply":"2025-12-27T00:46:35.483008Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"2025-12-27 00:45:52.804101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766796352.826913     534 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766796352.833504     534 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766796352.851375     534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766796352.851403     534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766796352.851407     534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766796352.851410     534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n[INFO] Loading tokenizer for Mistral...\n[INFO] Loading Mistral-7B-v0.2 in 4-bit...\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:14<00:00,  4.95s/it]\n[INFO] LLM model loaded successfully on cuda:0\n[INFO] Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded\nDevice set to use cuda:0\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n/kaggle/working/pipeline.py:28: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n  vector_db = Chroma(\n--- RAG Result ---\nQuestion: Which country left the Commonwealthin 1972 and rejoined in 1989?\nAnswer: Not found in context\nLatency: 8531 ms\n\n--- Context Found (Snippets) ---\nformally relinquished rights 15 March 1991 Ghana: 6 March 1957 (from UK) Gibraltar : none (overseas territory of the UK) Greece: 1829 (from the Ottoman Empire) Greenland: none (part of the Kingdom of Denmark; self-governing overseas administrative division of Denmark since 1979) Grenada: 7 February 1974 (from UK) Guadeloupe: none (overseas department of France) Guam: none (territory of the US) Guatemala: 15 September 1821 (from Spain) Guernsey: none (British crown dependency) Guinea: 2 October 1\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# APP","metadata":{}},{"cell_type":"code","source":"%%writefile app.py\nimport time\nimport json\nimport requests\nimport nest_asyncio\nimport uvicorn\nfrom threading import Thread\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom pipeline import run_rag   \n\napp = FastAPI()\n\nclass QueryRequest(BaseModel):\n    question: str\n\nclass QueryResponse(BaseModel):\n    question: str\n    answer: str\n    retrieved_context: str \n    latency_ms: int\n\n@app.post(\"/query\", response_model=QueryResponse)\ndef query_endpoint(req: QueryRequest):\n    result = run_rag(req.question)\n    return QueryResponse(\n        question=result.get(\"question\", req.question),\n        answer=result.get(\"answer\", \"No answer generated\"),\n        retrieved_context=result.get(\"retrieved_context\", \"\"),\n        latency_ms=result.get(\"latency_ms\", 0)\n    )\n\nnest_asyncio.apply()\n\ndef run_api():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8050, log_level=\"error\")\n\nthread = Thread(target=run_api, daemon=True)\nthread.start()\n\nprint(\"[INFO] Waiting for server to stabilize...\")\ntime.sleep(15) \n\nurl = \"http://127.0.0.1:8050/query\"\npayload = {\"question\": \"Which country left the Commonwealthin 1972 and rejoined in 1989?\"}\n\ntry:\n    resp = requests.post(url, json=payload)\n    print(\"POST status:\", resp.status_code)\n    if resp.status_code == 200:\n        print(\"POST response:\")\n        print(json.dumps(resp.json(), indent=4))\n    else:\n        print(\"Error Response:\", resp.text)\nexcept Exception as e:\n    print(f\"[ERROR] Connection failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:46:35.484966Z","iopub.execute_input":"2025-12-27T00:46:35.485311Z","iopub.status.idle":"2025-12-27T00:46:35.491666Z","shell.execute_reply.started":"2025-12-27T00:46:35.485271Z","shell.execute_reply":"2025-12-27T00:46:35.491081Z"}},"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!python app.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T00:46:35.492531Z","iopub.execute_input":"2025-12-27T00:46:35.492760Z","iopub.status.idle":"2025-12-27T00:47:36.578837Z","shell.execute_reply.started":"2025-12-27T00:46:35.492737Z","shell.execute_reply":"2025-12-27T00:47:36.577934Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"2025-12-27 00:46:40.140243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766796400.161364     599 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766796400.168217     599 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766796400.184573     599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766796400.184603     599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766796400.184607     599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766796400.184610     599 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n[INFO] Loading tokenizer for Mistral...\n[INFO] Loading Mistral-7B-v0.2 in 4-bit...\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:14<00:00,  4.77s/it]\n[INFO] LLM model loaded successfully on cuda:0\n[INFO] Embedding model 'sentence-transformers/all-MiniLM-L6-v2' loaded\nDevice set to use cuda:0\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n/kaggle/working/pipeline.py:28: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n  vector_db = Chroma(\n[INFO] Waiting for server to stabilize...\nPOST status: 200\nPOST response:\n{\n    \"question\": \"Which country left the Commonwealthin 1972 and rejoined in 1989?\",\n    \"answer\": \"Not found in context\",\n    \"retrieved_context\": \"formally relinquished rights 15 March 1991 Ghana: 6 March 1957 (from UK) Gibraltar : none (overseas territory of the UK) Greece: 1829 (from the Ottoman Empire) Greenland: none (part of the Kingdom of Denmark; self-governing overseas administrative division of Denmark since 1979) Grenada: 7 February 1974 (from UK) Guadeloupe: none (overseas department of France) Guam: none (territory of the US) Guatemala: 15 September 1821 (from Spain) Guernsey: none (British crown dependency) Guinea: 2 October 1958 (from France) Guinea-Bissau: 24 September 1973 (unilaterally declared by Guinea-Bissau); 10 September 1974 (recognized by Portugal) Guyana: 26 May 1966 (from UK) Haiti: 1 January 1804 (from France) Honduras: 15\\nformally relinquished rights 15 March 1991 Ghana: 6 March 1957 (from UK) Gibraltar : none (overseas territory of the UK) Greece: 1829 (from the Ottoman Empire) Greenland: none (part of the Kingdom of Denmark; self-governing overseas administrative division of Denmark since 1979) Grenada: 7 February 1974 (from UK) Guadeloupe: none (overseas department of France) Guam: none (territory of the US) Guatemala: 15 September 1821 (from Spain) Guernsey: none (British crown dependency) Guinea: 2 October 1958 (from France) Guinea-Bissau: 24 September 1973 (unilaterally declared by Guinea-Bissau); 10 September 1974 (recognized by Portugal) Guyana: 26 May 1966 (from UK) Haiti: 1 January 1804 (from France) Honduras: 15\\nformally relinquished rights 15 March 1991 Ghana: 6 March 1957 (from UK) Gibraltar : none (overseas territory of the UK) Greece: 1829 (from the Ottoman Empire) Greenland: none (part of the Kingdom of Denmark; self-governing overseas administrative division of Denmark since 1979) Grenada: 7 February 1974 (from UK) Guadeloupe: none (overseas department of France) Guam: none (territory of the US) Guatemala: 15 September 1821 (from Spain) Guernsey: none (British crown dependency) Guinea: 2 October 1958 (from France) Guinea-Bissau: 24 September 1973 (unilaterally declared by Guinea-Bissau); 10 September 1974 (recognized by Portugal) Guyana: 26 May 1966 (from UK) Haiti: 1 January 1804 (from France) Honduras: 15\\nMacmillan announced its decision to seek membership in the European Economic Community . Because of French opposition as well as Britain's request for special considerations for the countries of the Commonwealth and of EFTA, agreement on British entry was not reached until 1971. Britain finally entered what had become the European Community (now the European Union [EU]) in Jan., 1973. Labour returned to power in 1964 under Harold Wilson , and the steel industry was renationalized. The country faced the compound economic problems of a very unfavorable balance of trade, the instability of the pound sterling, a lagging rate of economic growth, and inflationary wages and prices. A number of sterling crises were followed by government controls and cutbacks. Britain supported U.S. policy in Vietnam. The policy of granting independence to colonial possessions continued; however, Rhodesia (see Zimbabwe ) became a problem when its government, representing only the white minority, unilaterally declared its independence in 1965. Another problem was Spain's demand for the return of Gibraltar. A major crisis erupted in Northern Ireland in late 1968 when\\nMacmillan announced its decision to seek membership in the European Economic Community . Because of French opposition as well as Britain's request for special considerations for the countries of the Commonwealth and of EFTA, agreement on British entry was not reached until 1971. Britain finally entered what had become the European Community (now the European Union [EU]) in Jan., 1973. Labour returned to power in 1964 under Harold Wilson , and the steel industry was renationalized. The country faced the compound economic problems of a very unfavorable balance of trade, the instability of the pound sterling, a lagging rate of economic growth, and inflationary wages and prices. A number of sterling crises were followed by government controls and cutbacks. Britain supported U.S. policy in Vietnam. The policy of granting independence to colonial possessions continued; however, Rhodesia (see Zimbabwe ) became a problem when its government, representing only the white minority, unilaterally declared its independence in 1965. Another problem was Spain's demand for the return of Gibraltar. A major crisis erupted in Northern Ireland in late 1968 when\",\n    \"latency_ms\": 8548\n}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport time\nimport re\nfrom pipeline import run_rag\n\nevaluation_set = [\n    {\"question\": \"Which number Beethoven symphony is known as 'The Pastoral'?\", \"ground_truth\": \"Sixth\"},\n    {\"question\": \"Miami Beach in Florida borders which ocean?\", \"ground_truth\": \"Atlantic\"},\n    {\"question\": \"What is the name of the perfume launched by British boyband JLS in January 2013?\", \"ground_truth\": \"Love\"},\n    {\"question\": \"Caroline of Brunswick was the queen consort of which British King?\", \"ground_truth\": \"George IV\"},\n    {\"question\": \"What is the official march of the Royal Navy?\", \"ground_truth\": \"Heart of Oak\"},\n    {\"question\": \"Technically a shoal of fish becomes a school of fish when it is?\", \"ground_truth\": \"Swimming in the same direction\"},\n    {\"question\": \"On which island was the famous photograph taken showing US Marines raising the US flag over Mt Suribachi in February 1945?\", \"ground_truth\": \"Iwo Jima\"},\n    {\"question\": \"What was the first name of the character played by John Travolta in Saturday Night Fever?\", \"ground_truth\": \"Tony (Manero)\"},\n    {\"question\": \"Jonas Salk developed a vaccine against what?\", \"ground_truth\": \"Polio\"},\n    {\"question\": \"Who is said to have cut the Gordian Knot?\", \"ground_truth\": \"Alexander the Great\"},\n    {\"question\": \"The Italian cheese called dolcelatte translates into English as what?\", \"ground_truth\": \"Sweet milk\"},\n    {\"question\": \"What is the title of the last Harry Potter novel, published in 2007?\", \"ground_truth\": \"Harry Potter and the Deathly Hallows\"},\n    {\"question\": \"Who was the first professional cricketer to captain England?\", \"ground_truth\": \"Len Hutton\"},\n    {\"question\": \"Which country left the Commonwealth in 1972 and rejoined in 1989?\", \"ground_truth\": \"Pakistan\"},\n    {\"question\": \"Wisent is an alternative name for which animal?\", \"ground_truth\": \"(European) Bison\"},\n    {\"question\": \"The site of Carthage is now in a suburb of which modem capital city?\", \"ground_truth\": \"Tunis\"},\n    {\"question\": \"In which country is the annual International Alphorn Festival held?\", \"ground_truth\": \"SWITZERLAND\"},\n    {\"question\": \"David Balfour and Alan Breck are characters in books by which author?\", \"ground_truth\": \"ROBERT LOUIS STEVENSON\"},\n    {\"question\": \"High Willhays is the highest point of what National Park?\", \"ground_truth\": \"DARTMOOR\"},\n    {\"question\": \"In 1973 the Paris Peace Accords were held in an attempt to end which war?\", \"ground_truth\": \"Vietnam\"}\n]\n\ndef normalize(text):\n    \"\"\"Remove punctuation, convert to lowercase, and strip spaces\"\"\"\n    return re.sub(r'[^\\w\\s]', '', text).lower().strip() if text else ''\n\ndef evaluate_system(test_set):\n    results = []\n    latencies = []\n\n    print(f\"Starting Evaluation on {len(test_set)} questions...\\n\")\n\n    for item in test_set:\n        q = item[\"question\"]\n        gt = item[\"ground_truth\"]\n\n        res = run_rag(q)\n        generated_ans = res.get(\"answer\", \"\").strip()\n        context = res.get(\"retrieved_context\", \"\")\n        latency = res.get(\"latency_ms\", 0)\n\n        latencies.append(latency)\n\n        # Normalize for comparison\n        gen_norm = normalize(generated_ans)\n        gt_norm = normalize(gt)\n        context_norm = normalize(context)\n\n        relevance = \"Yes\" if gt_norm in context_norm else \"No\"\n        context_ok = \"Yes\" if gt_norm in context_norm else \"No\"\n\n        # Evaluate correctness\n        if generated_ans == \"Not found in context\":\n            status = \"Incorrect\"\n        elif not context_ok:\n            status = \"Incorrect\"\n        elif gen_norm == gt_norm:\n            status = \"Correct\"\n        elif gt_norm in gen_norm or gen_norm in gt_norm:\n            status = \"Partially Correct\"\n        else:\n            status = \"Incorrect\"\n\n        results.append({\n            \"Question\": q,\n            \"Ground Truth\": gt,\n            \"RAG Answer\": generated_ans,\n            \"Context Correct?\": context_ok,\n            \"Evaluation Status\": status,\n            \"Latency (ms)\": latency,\n            \"Relevance\": relevance\n        })\n\n    df = pd.DataFrame(results)\n    accuracy = df[\"Evaluation Status\"].isin([\"Correct\", \"Partially Correct\"]).sum() / len(df) * 100\n    avg_latency = round(sum(latencies) / len(latencies), 2)\n    relevance_pct = (df[\"Relevance\"] == \"Yes\").sum() / len(df) * 100\n\n    print(\"\\n📊 Evaluation Summary\")\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    print(f\"Average Latency: {avg_latency} ms\")\n    print(f\"Relevance: {relevance_pct:.2f}%\")\n\n    return df\n\n# Run evaluation\ndf_final_results = evaluate_system(evaluation_set)\ndf_final_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T01:09:38.045281Z","iopub.execute_input":"2025-12-27T01:09:38.045913Z","iopub.status.idle":"2025-12-27T01:11:00.924045Z","shell.execute_reply.started":"2025-12-27T01:09:38.045868Z","shell.execute_reply":"2025-12-27T01:11:00.923412Z"}},"outputs":[{"name":"stdout","text":"Starting Evaluation on 20 questions...\n\n\n📊 Evaluation Summary\nAccuracy: 55.00%\nAverage Latency: 4142.1 ms\nRelevance: 50.00%\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                             Question  \\\n0   Which number Beethoven symphony is known as 'T...   \n1         Miami Beach in Florida borders which ocean?   \n2   What is the name of the perfume launched by Br...   \n3   Caroline of Brunswick was the queen consort of...   \n4       What is the official march of the Royal Navy?   \n5   Technically a shoal of fish becomes a school o...   \n6   On which island was the famous photograph take...   \n7   What was the first name of the character playe...   \n8        Jonas Salk developed a vaccine against what?   \n9           Who is said to have cut the Gordian Knot?   \n10  The Italian cheese called dolcelatte translate...   \n11  What is the title of the last Harry Potter nov...   \n12  Who was the first professional cricketer to ca...   \n13  Which country left the Commonwealth in 1972 an...   \n14    Wisent is an alternative name for which animal?   \n15  The site of Carthage is now in a suburb of whi...   \n16  In which country is the annual International A...   \n17  David Balfour and Alan Breck are characters in...   \n18  High Willhays is the highest point of what Nat...   \n19  In 1973 the Paris Peace Accords were held in a...   \n\n                            Ground Truth              RAG Answer  \\\n0                                  Sixth                   Sixth   \n1                               Atlantic          Atlantic Ocean   \n2                                   Love    Not found in context   \n3                              George IV               George IV   \n4                           Heart of Oak    Not found in context   \n5         Swimming in the same direction    Not found in context   \n6                               Iwo Jima                Iwo Jima   \n7                          Tony (Manero)             Tony Manero   \n8                                  Polio                   Polio   \n9                    Alexander the Great     Alexander the Great   \n10                            Sweet milk       Sweet blue cheese   \n11  Harry Potter and the Deathly Hallows         Deathly Hallows   \n12                            Len Hutton           Mike Brearley   \n13                              Pakistan          Bahamas (left)   \n14                      (European) Bison                 Aurochs   \n15                                 Tunis                   Tunis   \n16                           SWITZERLAND    Not found in context   \n17                ROBERT LOUIS STEVENSON  Robert Louis Stevenson   \n18                              DARTMOOR     Blackfoot Mountains   \n19                               Vietnam             Vietnam War   \n\n   Context Correct?  Evaluation Status  Latency (ms) Relevance  \n0               Yes            Correct          2019       Yes  \n1               Yes  Partially Correct          2488       Yes  \n2                No          Incorrect          6056        No  \n3               Yes            Correct          2438       Yes  \n4                No          Incorrect          4867        No  \n5                No          Incorrect          8709        No  \n6               Yes            Correct          2544       Yes  \n7               Yes            Correct          4648       Yes  \n8               Yes            Correct          6224       Yes  \n9               Yes            Correct          2382       Yes  \n10               No          Incorrect          8541        No  \n11               No  Partially Correct          2461        No  \n12               No          Incorrect          2448        No  \n13              Yes          Incorrect          6809       Yes  \n14               No          Incorrect          8464        No  \n15              Yes            Correct          2300       Yes  \n16               No          Incorrect          2826        No  \n17               No            Correct          2603        No  \n18               No          Incorrect          1684        No  \n19              Yes  Partially Correct          2331       Yes  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Ground Truth</th>\n      <th>RAG Answer</th>\n      <th>Context Correct?</th>\n      <th>Evaluation Status</th>\n      <th>Latency (ms)</th>\n      <th>Relevance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Which number Beethoven symphony is known as 'T...</td>\n      <td>Sixth</td>\n      <td>Sixth</td>\n      <td>Yes</td>\n      <td>Correct</td>\n      <td>2019</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Miami Beach in Florida borders which ocean?</td>\n      <td>Atlantic</td>\n      <td>Atlantic Ocean</td>\n      <td>Yes</td>\n      <td>Partially Correct</td>\n      <td>2488</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is the name of the perfume launched by Br...</td>\n      <td>Love</td>\n      <td>Not found in context</td>\n      <td>No</td>\n      <td>Incorrect</td>\n      <td>6056</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Caroline of Brunswick was the queen consort of...</td>\n      <td>George IV</td>\n      <td>George IV</td>\n      <td>Yes</td>\n      <td>Correct</td>\n      <td>2438</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the official march of the Royal Navy?</td>\n      <td>Heart of Oak</td>\n      <td>Not found in context</td>\n      <td>No</td>\n      <td>Incorrect</td>\n      <td>4867</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Technically a shoal of fish becomes a school o...</td>\n      <td>Swimming in the same direction</td>\n      <td>Not found in context</td>\n      <td>No</td>\n      <td>Incorrect</td>\n      <td>8709</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>On which island was the famous photograph take...</td>\n      <td>Iwo Jima</td>\n      <td>Iwo Jima</td>\n      <td>Yes</td>\n      <td>Correct</td>\n      <td>2544</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>What was the first name of the character playe...</td>\n      <td>Tony (Manero)</td>\n      <td>Tony Manero</td>\n      <td>Yes</td>\n      <td>Correct</td>\n      <td>4648</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Jonas Salk developed a vaccine against what?</td>\n      <td>Polio</td>\n      <td>Polio</td>\n      <td>Yes</td>\n      <td>Correct</td>\n      <td>6224</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Who is said to have cut the Gordian Knot?</td>\n      <td>Alexander the Great</td>\n      <td>Alexander the Great</td>\n      <td>Yes</td>\n      <td>Correct</td>\n      <td>2382</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The Italian cheese called dolcelatte translate...</td>\n      <td>Sweet milk</td>\n      <td>Sweet blue cheese</td>\n      <td>No</td>\n      <td>Incorrect</td>\n      <td>8541</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>What is the title of the last Harry Potter nov...</td>\n      <td>Harry Potter and the Deathly Hallows</td>\n      <td>Deathly Hallows</td>\n      <td>No</td>\n      <td>Partially Correct</td>\n      <td>2461</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Who was the first professional cricketer to ca...</td>\n      <td>Len Hutton</td>\n      <td>Mike Brearley</td>\n      <td>No</td>\n      <td>Incorrect</td>\n      <td>2448</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Which country left the Commonwealth in 1972 an...</td>\n      <td>Pakistan</td>\n      <td>Bahamas (left)</td>\n      <td>Yes</td>\n      <td>Incorrect</td>\n      <td>6809</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Wisent is an alternative name for which animal?</td>\n      <td>(European) Bison</td>\n      <td>Aurochs</td>\n      <td>No</td>\n      <td>Incorrect</td>\n      <td>8464</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>The site of Carthage is now in a suburb of whi...</td>\n      <td>Tunis</td>\n      <td>Tunis</td>\n      <td>Yes</td>\n      <td>Correct</td>\n      <td>2300</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>In which country is the annual International A...</td>\n      <td>SWITZERLAND</td>\n      <td>Not found in context</td>\n      <td>No</td>\n      <td>Incorrect</td>\n      <td>2826</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>David Balfour and Alan Breck are characters in...</td>\n      <td>ROBERT LOUIS STEVENSON</td>\n      <td>Robert Louis Stevenson</td>\n      <td>No</td>\n      <td>Correct</td>\n      <td>2603</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>High Willhays is the highest point of what Nat...</td>\n      <td>DARTMOOR</td>\n      <td>Blackfoot Mountains</td>\n      <td>No</td>\n      <td>Incorrect</td>\n      <td>1684</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>In 1973 the Paris Peace Accords were held in a...</td>\n      <td>Vietnam</td>\n      <td>Vietnam War</td>\n      <td>Yes</td>\n      <td>Partially Correct</td>\n      <td>2331</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20}]}